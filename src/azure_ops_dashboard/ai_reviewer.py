"""Step10: Azure Ops Dashboard â€” AI ãƒ¬ãƒ“ãƒ¥ãƒ¼ & ãƒ¬ãƒãƒ¼ãƒˆ (GitHub Copilot SDK)

Collect ã—ãŸãƒªã‚½ãƒ¼ã‚¹ä¸€è¦§ã‚’ Copilot SDK ã«é€ã‚Šã€
æ§‹æˆã®ãƒ¬ãƒ“ãƒ¥ãƒ¼çµæœã‚„å„ç¨®ãƒ¬ãƒãƒ¼ãƒˆï¼ˆæ—¥æœ¬èªï¼‰ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§è¿”ã™ã€‚
ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè¨­å®šã¨ã‚«ã‚¹ã‚¿ãƒ æŒ‡ç¤ºã«å¯¾å¿œã€‚
"""

from __future__ import annotations

import asyncio
import concurrent.futures
import json
import sys
import threading
import re
import time
from pathlib import Path
from typing import Any, Callable, Optional

_COPILOT_IMPORT_ERROR: str | None = None
try:
    from copilot import CopilotClient  # type: ignore
except Exception as exc:
    CopilotClient = None  # type: ignore[assignment]
    _COPILOT_IMPORT_ERROR = f"{type(exc).__name__}: {exc}"

from .app_paths import (
    bundled_templates_dir,
    copilot_cli_path,
    ensure_user_dirs,
    template_search_dirs,
)
from .docs_enricher import (
    cost_search_queries,
    enrich_with_docs,
    security_search_queries,
)
from .i18n import t as _t, get_language


_TOOL_INPUT_BLOCK_RE = re.compile(
    r"<tool_input\b[^>]*>\s*(.*?)\s*</tool_input>",
    re.IGNORECASE | re.DOTALL,
)

_JSON_DECODER = json.JSONDecoder()


def _looks_like_markdown_report(text: str) -> bool:
    if not text:
        return False
    trimmed = text.strip()
    if not trimmed:
        return False
    # Integrated reports are expected to have headings.
    return any(l.lstrip().startswith("#") for l in trimmed.splitlines() if l.strip())


def _extract_jsonish_string_field(payload: str, field_name: str) -> str | None:
    """å£Šã‚ŒãŸ JSON ã‹ã‚‰ã§ã‚‚ field ã®æ–‡å­—åˆ—å€¤ã‚’ãƒ™ã‚¹ãƒˆã‚¨ãƒ•ã‚©ãƒ¼ãƒˆã§æŠœãå‡ºã™ã€‚

    å…¸å‹ä¾‹:
      {"filePath":"x.md","content":"# Title\n..."}

    ãƒ¢ãƒ‡ãƒ«ãŒ JSON ã®æ–‡å­—åˆ—ä¸­æ”¹è¡Œã‚’ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã›ãšã«å‡ºã—ã¦ã—ã¾ã†ã¨ json.loads ãŒå¤±æ•—ã™ã‚‹ãŸã‚ã€
    `"content":"..."}` ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å„ªå…ˆã—ã¦æŠœãå–ã‚‹ã€‚
    """
    if not payload or not field_name:
        return None

    def _unescape_backslash_sequences(s: str) -> str:
        buf: list[str] = []
        escape = False
        for ch in s:
            if escape:
                if ch == "n":
                    buf.append("\n")
                elif ch == "r":
                    buf.append("\r")
                elif ch == "t":
                    buf.append("\t")
                elif ch == '"':
                    buf.append('"')
                elif ch == "\\":
                    buf.append("\\")
                else:
                    buf.append(ch)
                escape = False
                continue
            if ch == "\\":
                escape = True
                continue
            buf.append(ch)
        if escape:
            buf.append("\\")
        return "".join(buf)

    # 1) content ãŒã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆæœ«å°¾ï¼ˆ"..." }ï¼‰ã«ã‚ã‚‹ã‚±ãƒ¼ã‚¹ã‚’å„ªå…ˆï¼ˆæœ€ã‚‚æˆåŠŸç‡ãŒé«˜ã„ï¼‰
    tail_pat = re.compile(
        rf"[\"']{re.escape(field_name)}[\"']\s*:\s*\"(.*)\"\s*}}\s*$",
        re.IGNORECASE | re.DOTALL,
    )
    m = tail_pat.search(payload.strip())
    if m:
        s = m.group(1)
        if s and s.strip():
            return _unescape_backslash_sequences(s)

    # 2) ãã‚Œä»¥å¤–: "field":"..." ã‚’ 1 ã¤åˆ†ã ã‘ state-machine ã§æŠœãï¼ˆé€”ä¸­ã«æœªã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã® " ãŒã‚ã‚‹ã¨æ‰“ã¡åˆ‡ã‚Šï¼‰
    head_pat = re.compile(
        rf"[\"']{re.escape(field_name)}[\"']\s*:\s*\"",
        re.IGNORECASE,
    )
    m2 = head_pat.search(payload)
    if not m2:
        return None

    i = m2.end()
    buf: list[str] = []
    escape = False
    while i < len(payload):
        ch = payload[i]
        if escape:
            if ch == "n":
                buf.append("\n")
            elif ch == "r":
                buf.append("\r")
            elif ch == "t":
                buf.append("\t")
            elif ch == '"':
                buf.append('"')
            elif ch == "\\":
                buf.append("\\")
            else:
                buf.append(ch)
            escape = False
            i += 1
            continue

        if ch == "\\":
            escape = True
            i += 1
            continue

        if ch == '"':
            break

        buf.append(ch)
        i += 1

    out = "".join(buf)
    return out if out.strip() else None


def _best_effort_json_load(payload: str) -> Any | None:
    """JSON ã‚’ãƒ™ã‚¹ãƒˆã‚¨ãƒ•ã‚©ãƒ¼ãƒˆã§ parse ã™ã‚‹ã€‚

    - å…ˆé ­ã«ä½™è¨ˆãªæ–‡å­—ãŒã‚ã‚‹/æœ«å°¾ã«ã‚´ãƒŸãŒä»˜ãã‚±ãƒ¼ã‚¹ã§ã‚‚ã€å…ˆé ­ã® JSON ã‚’æ‹¾ãˆã‚‹ç¯„å›²ã§æ‹¾ã†ã€‚
    - å¤±æ•—ã—ãŸã‚‰ Noneã€‚
    """
    if not payload:
        return None
    s = payload.strip()
    if not s:
        return None

    # Common fence wrappers (best-effort)
    if s.startswith("```"):
        # ```json\n{...}\n```
        first_nl = s.find("\n")
        last_fence = s.rfind("```")
        if first_nl >= 0 and last_fence > first_nl:
            inner = s[first_nl + 1:last_fence].strip()
            if inner:
                s = inner

    try:
        return json.loads(s)
    except Exception:
        pass

    try:
        obj, _idx = _JSON_DECODER.raw_decode(s)
        return obj
    except Exception:
        return None


def _score_markdown_candidate(text: str) -> int:
    if not text:
        return 0
    trimmed = text.strip()
    if not trimmed:
        return 0

    score = 0
    head_lines = trimmed.splitlines()[:30]
    heading_count = sum(1 for l in head_lines if l.lstrip().startswith("#"))
    score += min(40, heading_count * 8)
    if "\n|" in trimmed and "|" in trimmed:
        score += 10
    score += min(40, len(trimmed) // 400)
    lowered = trimmed.lower()
    if "<tool_call" in lowered or "<tool_calls" in lowered or "<tool_input" in lowered:
        score -= 50
    return score


def _extract_markdown_from_tool_input(text: str) -> str | None:
    """tool-call trace ã‹ã‚‰ Markdown æœ¬æ–‡ã‚’æ•‘å‡ºã™ã‚‹ï¼ˆãƒ™ã‚¹ãƒˆã‚¨ãƒ•ã‚©ãƒ¼ãƒˆï¼‰ã€‚

    ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ãŒ `<tool_call>...<tool_input>{"content": "..."}</tool_input>...` ã®å½¢å¼
    ã«ãªã£ãŸå ´åˆã€ãƒ„ãƒ¼ãƒ«ãƒ–ãƒ­ãƒƒã‚¯ã‚’ä¸¸ã”ã¨é™¤å»ã™ã‚‹ã¨æœ¬æ–‡ã‚‚æ¶ˆãˆã‚‹ãŸã‚ã€content ã‚’æŠ½å‡ºã—ã¦è¿”ã™ã€‚
    """
    if not text or "<tool_input" not in text:
        return None

    candidates: list[str] = []
    for m in _TOOL_INPUT_BLOCK_RE.finditer(text):
        payload = (m.group(1) or "").strip()
        if not payload:
            continue
        obj = _best_effort_json_load(payload)
        if obj is None:
            # Broken JSON fallback: try to extract a plausible Markdown string.
            for key in ("content", "markdown", "text"):
                extracted = _extract_jsonish_string_field(payload, key)
                if extracted and extracted.strip():
                    candidates.append(extracted)
                    break
            else:
                # Some models put raw Markdown directly inside <tool_input>.
                if _looks_like_markdown_report(payload):
                    candidates.append(payload)
            continue

        if isinstance(obj, dict):
            for key in ("content", "markdown", "text"):
                content = obj.get(key)
                if isinstance(content, str) and content.strip():
                    candidates.append(content)
                    break
            else:
                args = obj.get("arguments")
                if isinstance(args, dict):
                    for key in ("content", "markdown", "text"):
                        content2 = args.get(key)
                        if isinstance(content2, str) and content2.strip():
                            candidates.append(content2)
                            break

    if not candidates:
        return None
    return max(candidates, key=_score_markdown_candidate)


def _sanitize_ai_markdown(text: str) -> str:
    """AI å‡ºåŠ›ã«æ··å…¥ã—ãŒã¡ãªãƒ¡ã‚¿æƒ…å ±ã‚’é™¤å»ã—ã€ãƒ¬ãƒãƒ¼ãƒˆæœ¬æ–‡ã«å¯„ã›ã‚‹ã€‚"""
    if not text:
        return text

    extracted = _extract_markdown_from_tool_input(text)
    # Quality gate: extracted must be substantial (not just thinking preamble)
    # to prevent adopting a short "Let me examine..." snippet over the full output.
    if extracted and _looks_like_markdown_report(extracted):
        ex_lines = [l for l in extracted.splitlines() if l.strip()]
        ex_headings = sum(1 for l in ex_lines if l.lstrip().startswith("#"))
        if len(extracted.strip()) >= 300 and ex_headings >= 2:
            if _score_markdown_candidate(extracted) > _score_markdown_candidate(text):
                text = extracted

    lines = text.splitlines()
    sanitized: list[str] = []
    skip_tag: str | None = None
    in_code_fence = False

    tool_trace_tags_always = (
        "tool_call",
        "tool_call_result",
        "tool_result",
        "tool_calls",
        "tool_results",
        "tool_input",
        "tool_name",
        "FileReadResult",
        "FileWriteResult",
    )
    # NOTE:
    # These tags are too generic to treat as multi-line blocks. If we set skip_tag on an
    # opening tag without a matching close tag (which happens in real model outputs), we can
    # accidentally drop the entire report and cause downstream failures like "no_heading".
    # For these, we only drop the tag lines (outside code fences) and keep the rest.
    tool_trace_line_tags_outside_fence = (
        "parameters",
        "parameter",
        "result",
    )

    for line in lines:
        stripped = line.strip()

        if stripped.startswith("```"):
            in_code_fence = not in_code_fence
            sanitized.append(line)
            continue

        # Skip inside tool-trace blocks (also inside code fences).
        if skip_tag is not None:
            if re.search(rf"</\s*{re.escape(skip_tag)}\s*>", stripped, re.IGNORECASE):
                skip_tag = None
            continue

        # 1) Drop multi-line tool-trace blocks (also inside code fences).
        start_tag: str | None = None
        for tag in tool_trace_tags_always:
            if re.search(rf"<\s*{re.escape(tag)}\b", stripped, re.IGNORECASE):
                start_tag = tag
                break
        if start_tag is not None:
            # Handle "<tag>... </tag>" on the same line safely.
            if re.search(rf"</\s*{re.escape(start_tag)}\s*>", stripped, re.IGNORECASE):
                continue
            skip_tag = start_tag
            continue

        # 2) Drop generic tool-ish tag lines (outside code fences) but do NOT enter skip mode.
        #    This prevents swallowing the report when the close tag is missing.
        if not in_code_fence:
            for tag in tool_trace_line_tags_outside_fence:
                if re.search(rf"<\s*/?\s*{re.escape(tag)}\b", stripped, re.IGNORECASE):
                    # Only drop the tag line itself; keep the rest of the report.
                    start_tag = tag
                    break
            if start_tag is not None:
                continue

        if stripped.startswith("Tool summary:"):
            continue

        sanitized.append(line)

    out = "\n".join(sanitized).strip()

    # Reconsider: if line-by-line result is too short but we had a substantial extracted
    # candidate that failed the strict quality gate, prefer it over a near-empty result.
    # Only adopt if extracted is also at least as long (avoids replacing useful post-tool
    # text with a short "thinking" snippet from tool_input).
    if extracted and len(out.strip()) < 300:
        ex_score = _score_markdown_candidate(extracted)
        out_score = _score_markdown_candidate(out)
        if ex_score > out_score and len(extracted.strip()) >= len(out.strip()):
            out = extracted.strip()

    # è¦‹å‡ºã—(#)é–‹å§‹ã«å¯„ã›ã‚‹ï¼ˆãŸã ã— Target metadata ã¯è¨±å®¹ï¼‰
    out_lines = out.splitlines()
    heading_idx = next((i for i, l in enumerate(out_lines) if l.lstrip().startswith("#")), None)
    if heading_idx is not None and heading_idx > 0:
        pre = [l.strip() for l in out_lines[:heading_idx] if l.strip()]
        allowed_prefixes = (
            "**Target Subscription**:",
            "**Target Resource Group**:",
            "**å¯¾è±¡ã‚µãƒ–ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³**:",
            "**å¯¾è±¡ãƒªã‚½ãƒ¼ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—**:",
            "**ã‚µãƒ–ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³**:",
            "**ãƒªã‚½ãƒ¼ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—**:",
        )
        if not (pre and all(any(p.startswith(ap) for ap in allowed_prefixes) for p in pre)):
            out = "\n".join(out_lines[heading_idx:]).strip()

    return out


def _system_prompt_drawio() -> str:
    """draw.io å›³ç”Ÿæˆï¼ˆmxfile XMLï¼‰ç”¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã€‚

    æ³¨æ„: drawio ç”Ÿæˆã§ã¯ Markdown ã‚’è¦æ±‚ã™ã‚‹ã¨å£Šã‚Œã‚„ã™ã„ã®ã§ã€
    `AIReviewer.generate(..., append_language_instruction=False)` ã§å‘¼ã¶ã“ã¨ã€‚
    """

    # drawio_writer ã®ã‚¢ã‚¤ã‚³ãƒ³ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ AI ã«æ¸¡ã—ã¦ã€ã‚¿ã‚¤ãƒ—â†’ã‚¢ã‚¤ã‚³ãƒ³ã®ä¸€è²«æ€§ã‚’ä¸Šã’ã‚‹ã€‚
    # å¤±æ•—ã—ã¦ã‚‚å›³ç”Ÿæˆè‡ªä½“ã¯å¯èƒ½ãªã®ã§ã€import ã¯é…å»¶ + ãƒ™ã‚¹ãƒˆã‚¨ãƒ•ã‚©ãƒ¼ãƒˆã€‚
    icons: dict[str, str] = {}
    try:
        from . import drawio_writer

        icons = dict(getattr(drawio_writer, "_TYPE_ICONS", {}) or {})
    except Exception:
        icons = {}

    icons_json = json.dumps(icons, ensure_ascii=False, indent=2)

    if get_language() == "en":
        return f"""\
You are an expert draw.io (diagrams.net) diagram generator for Azure environments.

The user will provide PREPROCESSED Azure resources and relationships as JSON.
Note: Noise resources have been filtered and similar resources grouped BEFORE you receive the data.
Your task is to output a SINGLE valid draw.io mxfile XML â€” compact, readable, and professional.

â•â•â• CRITICAL OUTPUT RULES â•â•â•
- Output ONLY XML. No Markdown. No code fences. No explanations.
- The output must contain exactly one <mxfile> root element.

â•â•â• DO NOT OUTPUT AN EMPTY DIAGRAM â•â•â•
- You must create vertex nodes for ALL provided input nodes (use their cellId).
- Do not output placeholder comments like "content cells here".

â•â•â• XML STRUCTURE â•â•â•
- Include <mxCell id=\"0\"/> and <mxCell id=\"1\" parent=\"0\"/>.
- Nodes: <mxCell vertex=\"1\"> with child <mxGeometry ... as=\"geometry\"/>.
- Edges: <mxCell edge=\"1\" source=... target=...> referring to existing node ids.
- All mxCell ids must be UNIQUE.

â•â•â• ID RULES (VERY IMPORTANT) â•â•â•
- For input nodes: use node.cellId as the mxCell id.
- For input edges: use source="sourceCellId" target="targetCellId".
- Do not invent ids for resources.
- Container/title ids must not collide with node ids (use "c_" or "t_" prefix).

â•â•â• ICON RULES â•â•â•
- Azure icons: style='shape=image;aspect=fixed;image=img/lib/azure2/.../*.svg;...'
- NEVER use mxgraph.azure.* shapes â†’ REJECTED.
- NEVER use http/https image URLs.
- Typeâ†’icon mapping:
```json
{icons_json}
```

â•â•â• LAYOUT (CRITICAL â€” this is the most important section) â•â•â•

HIERARCHY (use swimlane containers, style="swimlane;..."):
  1. Azure (outermost) â†’ 2. Region â†’ 3. Resource Group â†’ 4. VNet â†’ 5. Subnet
  Omit levels with only 1 child (inline into parent).

CONTAINER SIZING:
  - Leaf nodes (icons): width=50 height=50
  - "+N more" summary labels: width=120 height=30, fontSize=10, italic
  - Subnet: pad 20px; VNet: pad 30px; RG: pad 40px

MULTI-COLUMN GRID:
  - Arrange containers/nodes in 2-4 column grids (NOT a single vertical stack).
  - For regions: place side-by-side if content is small, stack if large.
  - Small regions (â‰¤3 nodes): 200px wide. Large regions: up to 800px wide.

RESOURCE GROUP GROUPING:
  - Group nodes by resourceGroup within each region.
  - Use a dashed-border container (dashed=1) for each RG.
  - Label: "RG: <name>" with fontSize=11.

SIMILAR-RESOURCE GROUPS (nodes with name like "prefix... (+N more)"):
  - These are already grouped by preprocessing. Display the summary node as-is.
  - Place the summary node next to the representative nodes in a compact row.

LABELS:
  - Truncate labels over 22 chars: keep first 10 + "..." + last 8.
  - Example: "DefaultWorkspace-832c4080-..." â†’ "DefaultWor...080-..."
  - Use verticalLabelPosition=bottom for icon nodes.

CANVAS SIZING:
  - Target: width â‰¤ 1600px, height â‰¤ 2000px (compact!).
  - Adjust dynamically: ~100px per node height, ~200px per node width.
  - If nodes > 40: use smaller icons (width=40 height=40) and tighter spacing.

â•â•â• EDGE RULES â•â•â•
- Orthogonal routing: edgeStyle=orthogonalEdgeStyle;rounded=1;
- VNet peering: dashed=1;strokeWidth=2;strokeColor=#0078D4;
- Containment edges (in-subnet, contained-in): OMIT (parent relationship suffices).
- If resource is inside its container, do NOT draw an edge to it.

â•â•â• DATA FIDELITY â•â•â•
- Do not invent resources or relationships.
- If relationships are missing, omit edges rather than guessing.
"""

    return f"""\
ã‚ãªãŸã¯ Azure ç’°å¢ƒã® draw.io (diagrams.net) å›³ã‚’ç”Ÿæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚

ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰**å‰å‡¦ç†æ¸ˆã¿**ã® Azure ãƒªã‚½ãƒ¼ã‚¹ã¨é–¢ä¿‚æ€§ãŒ JSON ã§æä¾›ã•ã‚Œã¾ã™ã€‚
æ³¨æ„: ãƒã‚¤ã‚ºãƒªã‚½ãƒ¼ã‚¹ã¯é™¤å»æ¸ˆã¿ã€é¡ä¼¼ãƒªã‚½ãƒ¼ã‚¹ã¯ã‚°ãƒ«ãƒ¼ãƒ—åŒ–æ¸ˆã¿ã§ã™ã€‚
ã‚ãªãŸã®ã‚¿ã‚¹ã‚¯ã¯ã€ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã§è¦‹ã‚„ã™ã„ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ãª mxfile XML ã‚’1ã¤å‡ºåŠ›ã™ã‚‹ã“ã¨ã§ã™ã€‚

â•â•â• æœ€é‡è¦: å‡ºåŠ›ãƒ«ãƒ¼ãƒ« â•â•â•
- å‡ºåŠ›ã¯ XML ã®ã¿ã€‚Markdownç¦æ­¢ã€‚ã‚³ãƒ¼ãƒ‰ãƒ•ã‚§ãƒ³ã‚¹ç¦æ­¢ã€‚èª¬æ˜æ–‡ç¦æ­¢ã€‚
- å‡ºåŠ›ã¯ <mxfile> ãƒ«ãƒ¼ãƒˆè¦ç´ ã‚’1ã¤ã ã‘å«ã‚€ã“ã¨ã€‚

â•â•â• ç©ºå›³ã®ç¦æ­¢ â•â•â•
- å…¥åŠ›ãƒãƒ¼ãƒ‰å…¨ã¦ã® cellId ã«å¯¾å¿œã™ã‚‹ vertex ã‚’å¿…ãšç”Ÿæˆã™ã‚‹ã“ã¨ã€‚
- ã€Œcontent cells hereã€ç­‰ã®ãƒ—ãƒ¬ãƒ¼ã‚¹ãƒ›ãƒ«ãƒ€ã‚³ãƒ¡ãƒ³ãƒˆã¯ç¦æ­¢ã€‚

â•â•â• XML æ§‹é€  â•â•â•
- <mxCell id=\"0\"/> ã¨ <mxCell id=\"1\" parent=\"0\"/> ã‚’å¿…ãšå«ã‚ã‚‹ã€‚
- ãƒãƒ¼ãƒ‰: <mxCell vertex=\"1\"> + å­ã« <mxGeometry ... as=\"geometry\"/>ã€‚
- ã‚¨ãƒƒã‚¸: <mxCell edge=\"1\" source=... target=...>ã€æ—¢å­˜ãƒãƒ¼ãƒ‰ id ã‚’å‚ç…§ã€‚
- mxCell id ã¯å…¨ã¦ãƒ¦ãƒ‹ãƒ¼ã‚¯ã€‚

â•â•â• ID ãƒ«ãƒ¼ãƒ«ï¼ˆæœ€é‡è¦ï¼‰ â•â•â•
- å…¥åŠ›ãƒãƒ¼ãƒ‰: node.cellId ã‚’ mxCell id ã¨ã—ã¦ä½¿ç”¨ã€‚
- å…¥åŠ›ã‚¨ãƒƒã‚¸: source="sourceCellId" / target="targetCellId"ã€‚
- ãƒªã‚½ãƒ¼ã‚¹ id ã‚’æé€ ã—ãªã„ã€‚
- ã‚³ãƒ³ãƒ†ãƒŠ/ã‚¿ã‚¤ãƒˆãƒ« id ã¯ "c_" / "t_" ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã§ node.cellId ã¨è¡çªå›é¿ã€‚

â•â•â• ã‚¢ã‚¤ã‚³ãƒ³ â•â•â•
- style ã« 'shape=image;aspect=fixed;image=img/lib/azure2/.../*.svg;...' ã‚’ä½¿ç”¨ã€‚
- shape=mxgraph.azure.* ã¯ç¦æ­¢ â†’ ä¸åˆæ ¼ã€‚
- http/https ãƒªãƒ¢ãƒ¼ãƒˆç”»åƒURL ç¦æ­¢ã€‚
- ã‚¿ã‚¤ãƒ—â†’ã‚¢ã‚¤ã‚³ãƒ³å¯¾å¿œè¡¨:
```json
{icons_json}
```

â•â•â• ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆï¼ˆæœ€é‡è¦ â€” ã“ã®å›³ã®å“è³ªã‚’æ±ºã‚ã‚‹ï¼‰ â•â•â•

ã€éšå±¤æ§‹é€ ã€‘swimlane ã‚³ãƒ³ãƒ†ãƒŠä½¿ç”¨:
  1. Azureï¼ˆæœ€å¤–æ ï¼‰â†’ 2. Region â†’ 3. Resource Group â†’ 4. VNet â†’ 5. Subnet
  å­ãŒ1ã¤ã ã‘ã®éšå±¤ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆè¦ªã«ã‚¤ãƒ³ãƒ©ã‚¤ãƒ³å±•é–‹ï¼‰ã€‚

ã€ã‚³ãƒ³ãƒ†ãƒŠã‚µã‚¤ã‚ºã€‘
  - ãƒªãƒ¼ãƒ•ãƒãƒ¼ãƒ‰ï¼ˆã‚¢ã‚¤ã‚³ãƒ³ï¼‰: width=50 height=50
  - "+N more" ã‚µãƒãƒª: width=120 height=30, fontSize=10, italic
  - Subnet: 20pxä½™ç™½; VNet: 30pxä½™ç™½; RG: 40pxä½™ç™½

ã€ã‚°ãƒªãƒƒãƒ‰é…ç½®ï¼ˆç¸¦ä¸€åˆ—ç¦æ­¢ï¼‰ã€‘
  - ã‚³ãƒ³ãƒ†ãƒŠ/ãƒãƒ¼ãƒ‰ã¯ 2ï½4åˆ—ã®ã‚°ãƒªãƒƒãƒ‰ã«é…ç½®ã™ã‚‹ã€‚
  - å°ã•ã„ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ï¼ˆâ‰¤3ãƒãƒ¼ãƒ‰ï¼‰: å¹…200pxã€‚å¤§ãã„ãƒªãƒ¼ã‚¸ãƒ§ãƒ³: æœ€å¤§800pxã€‚
  - ãƒªãƒ¼ã‚¸ãƒ§ãƒ³åŒå£«: å†…å®¹ãŒå°ã•ã‘ã‚Œã°æ¨ªä¸¦ã³ã€å¤§ãã‘ã‚Œã°ç¸¦æ–¹å‘ã«é…ç½®ã€‚

ã€ãƒªã‚½ãƒ¼ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã«ã‚ˆã‚‹ã‚°ãƒ«ãƒ¼ãƒ”ãƒ³ã‚°ã€‘
  - å„ãƒªãƒ¼ã‚¸ãƒ§ãƒ³å†…ã§ãƒãƒ¼ãƒ‰ã‚’ resourceGroup ã”ã¨ã«ã¾ã¨ã‚ã‚‹ã€‚
  - ç ´ç·šã‚³ãƒ³ãƒ†ãƒŠ (dashed=1) ã§ RG ã‚’å›²ã‚€ã€‚
  - ãƒ©ãƒ™ãƒ«: "RG: <åå‰>" fontSize=11ã€‚

ã€é¡ä¼¼ãƒªã‚½ãƒ¼ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—ã€‘ï¼ˆåå‰ãŒ "prefix... (+N more)" ã®ãƒãƒ¼ãƒ‰ï¼‰:
  - å‰å‡¦ç†ã§ã‚°ãƒ«ãƒ¼ãƒ—åŒ–æ¸ˆã¿ã€‚ã‚µãƒãƒªãƒãƒ¼ãƒ‰ã¯ãã®ã¾ã¾è¡¨ç¤ºã€‚
  - ä»£è¡¨ãƒãƒ¼ãƒ‰ã®æ¨ªã«ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã«ä¸¦ã¹ã‚‹ã€‚

ã€ãƒ©ãƒ™ãƒ«çŸ­ç¸®ã€‘
  - 22æ–‡å­—è¶…: å…ˆé ­10æ–‡å­— + "..." + æœ«å°¾8æ–‡å­—ã€‚
  - ä¾‹: "DefaultWorkspace-832c4080-..." â†’ "DefaultWor...080-..."
  - ã‚¢ã‚¤ã‚³ãƒ³ãƒãƒ¼ãƒ‰ã¯ verticalLabelPosition=bottomã€‚

ã€ã‚­ãƒ£ãƒ³ãƒã‚¹ã‚µã‚¤ã‚ºï¼ˆã‚³ãƒ³ãƒ‘ã‚¯ãƒˆã«ï¼ï¼‰ã€‘
  - ç›®æ¨™: å¹… â‰¤ 1600pxã€é«˜ã• â‰¤ 2000pxã€‚
  - ãƒãƒ¼ãƒ‰æ•°ã«å¿œã˜ã¦å‹•çš„èª¿æ•´: ï½100px/ãƒãƒ¼ãƒ‰é«˜ã€ï½200px/ãƒãƒ¼ãƒ‰å¹…ã€‚
  - ãƒãƒ¼ãƒ‰ > 40 ã®å ´åˆ: å°ã‚¢ã‚¤ã‚³ãƒ³ (width=40 height=40) + é–“éš”ã‚’è©°ã‚ã‚‹ã€‚

â•â•â• ã‚¨ãƒƒã‚¸ â•â•â•
- ç›´è¡Œãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°: edgeStyle=orthogonalEdgeStyle;rounded=1;
- VNet peering: dashed=1;strokeWidth=2;strokeColor=#0078D4;
- åŒ…å«ã‚¨ãƒƒã‚¸ (in-subnet, contained-in): çœç•¥ï¼ˆparenté–¢ä¿‚ã§è¡¨ç¾æ¸ˆã¿ï¼‰ã€‚
- ãƒªã‚½ãƒ¼ã‚¹ãŒã‚³ãƒ³ãƒ†ãƒŠå†…ã«ã‚ã‚‹å ´åˆã€ãã‚Œã¸ã®ã‚¨ãƒƒã‚¸ã¯å¼•ã‹ãªã„ã€‚

â•â•â• ãƒ‡ãƒ¼ã‚¿å¿ å®Ÿæ€§ â•â•â•
- ãƒªã‚½ãƒ¼ã‚¹ã‚„é–¢ä¿‚æ€§ã‚’æé€ ã—ãªã„ã€‚
- é–¢ä¿‚æ€§ä¸è¶³ã®å ´åˆã€æ¨æ¸¬ã§ã‚¨ãƒƒã‚¸ã‚’å¼•ã‹ãšçœç•¥ã™ã‚‹ã€‚
"""


_MXFILE_RE = re.compile(r"(<mxfile[\s\S]*?</mxfile>)", re.IGNORECASE)


def _extract_mxfile_xml(text: str) -> str | None:
    """ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã‹ã‚‰ <mxfile>...</mxfile> ã‚’æŠ½å‡ºã™ã‚‹ï¼ˆãƒ™ã‚¹ãƒˆã‚¨ãƒ•ã‚©ãƒ¼ãƒˆï¼‰ã€‚"""
    if not text:
        return None

    s = text.strip()
    # Code fence ã‚’å‰¥ãŒã™ï¼ˆãƒ¢ãƒ‡ãƒ«ãŒãƒ«ãƒ¼ãƒ«ã‚’ç ´ã£ãŸå ´åˆã®æ•‘æ¸ˆï¼‰
    if s.startswith("```"):
        s = re.sub(r"^```[a-zA-Z0-9_-]*\n", "", s)
        s = re.sub(r"\n```$", "", s)
        s = s.strip()

    m = _MXFILE_RE.search(s)
    if m:
        xml = m.group(1).strip()
        # <?xml ...?> ãŒç›´å‰ã«ã‚ã‚Œã°å«ã‚ã‚‹
        xml_decl = s.lower().rfind("<?xml", 0, m.start(1))
        if xml_decl != -1:
            return (s[xml_decl:m.end(1)]).strip()
        return xml

    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ«ãƒ¼ãƒˆé–‹å§‹/çµ‚äº†ã§åˆ‡ã‚Šå‡ºã—
    start = s.lower().find("<mxfile")
    end = s.lower().rfind("</mxfile>")
    if start != -1 and end != -1:
        end2 = end + len("</mxfile>")
        return s[start:end2].strip()

    return None


def _approve_all(request: object) -> dict:
    """å…¨ã¦ã®ãƒ‘ãƒ¼ãƒŸãƒƒã‚·ãƒ§ãƒ³ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚’æ‰¿èªã™ã‚‹ã€‚"""
    return {"kind": "approved", "rules": []}


# ============================================================
# Session Hooksï¼ˆSDKæ¨å¥¨ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
# ============================================================

# èª­ã¿å–ã‚Šå°‚ç”¨ãƒ„ãƒ¼ãƒ«ã®ã¿è¨±å¯ï¼ˆå®‰å…¨æ€§å‘ä¸Šï¼‰
_ALLOWED_TOOLS = frozenset({
    "view", "read", "readFile", "search", "grep",
    "list", "ls", "find", "cat", "head", "tail",
    # Microsoft Docs MCP ãƒ„ãƒ¼ãƒ«ï¼ˆèª­ã¿å–ã‚Šå°‚ç”¨ï¼‰
    "microsoft_docs_search",
    "microsoft_docs_fetch",
    "microsoft_code_sample_search",
})


async def _on_pre_tool_use(input_data: dict, invocation: Any) -> dict:
    """ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œå‰ã®ãƒ•ãƒƒã‚¯: èª­ã¿å–ã‚Šå°‚ç”¨ãƒ„ãƒ¼ãƒ«ã®ã¿è¨±å¯ã€‚"""
    tool_name = input_data.get("toolName", "")
    # èª­ã¿å–ã‚Šç³»ã¯è¨±å¯ã€ãã‚Œä»¥å¤–ã¯æ‹’å¦
    if tool_name in _ALLOWED_TOOLS:
        decision = "allow"
    else:
        decision = "deny"
    return {
        "permissionDecision": decision,
        "modifiedArgs": input_data.get("toolArgs"),
    }


def _make_on_pre_tool_use(
    *,
    on_status: Callable[[str], None],
    run_debug: dict[str, Any],
) -> Callable:
    """è¦³æ¸¬ç”¨ã® on_pre_tool_use ãƒ•ãƒƒã‚¯ï¼ˆallow/deny ã‚’è¨˜éŒ²ï¼‰ã€‚"""

    async def _hook(input_data: dict, invocation: Any) -> dict:
        tool_name = str(input_data.get("toolName", "") or "")
        tool_args = input_data.get("toolArgs")

        decision = "allow" if tool_name in _ALLOWED_TOOLS else "deny"

        counts = run_debug.setdefault("tool_counts", {})
        key = tool_name or "(unknown)"
        entry = counts.setdefault(key, {"allow": 0, "deny": 0})
        entry[decision] = int(entry.get(decision, 0)) + 1
        run_debug["tool_total"] = int(run_debug.get("tool_total", 0)) + 1

        # docs MCP ãƒ„ãƒ¼ãƒ«ã ã‘ã¯ãƒ­ã‚°ã«ã‚‚å‡ºã™ï¼ˆãã®ä»–ã¯ãƒã‚¤ã‚ºã«ãªã‚Šã‚„ã™ã„ã®ã§æŠ‘åˆ¶ï¼‰
        if tool_name.startswith("microsoft_") or decision == "deny":
            on_status(f"Tool: {tool_name} => {decision}")

        return {
            "permissionDecision": decision,
            "modifiedArgs": tool_args,
        }

    return _hook


def _make_error_handler(
    on_status: Callable[[str], None],
    max_retry: int = 2,
    run_debug: dict[str, Any] | None = None,
) -> Callable:
    """ãƒªãƒˆãƒ©ã‚¤ä»˜ãã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ã‚’ç”Ÿæˆã€‚"""
    _retry_count: dict[str, int] = {}

    async def _on_error_occurred(input_data: dict, invocation: Any) -> dict:
        ctx = input_data.get("errorContext", "unknown")
        err = input_data.get("error", "")

        if run_debug is not None:
            errors = run_debug.setdefault("errors", [])
            s = str(err)
            errors.append({
                "context": str(ctx),
                "error": (s[:500] + "..." if len(s) > 500 else s),
            })

        key = f"{ctx}:{err}"
        _retry_count[key] = _retry_count.get(key, 0) + 1

        if _retry_count[key] <= max_retry:
            wait = RETRY_BACKOFF ** _retry_count[key]
            if get_language() == "en":
                on_status(f"AI error (retry {_retry_count[key]}/{max_retry}, waiting {wait:.0f}s): {err}")
            else:
                on_status(f"AI ã‚¨ãƒ©ãƒ¼ï¼ˆãƒªãƒˆãƒ©ã‚¤ {_retry_count[key]}/{max_retry}, {wait:.0f}s å¾…æ©Ÿï¼‰: {err}")
            await asyncio.sleep(wait)
            return {"errorHandling": "retry"}
        else:
            if get_language() == "en":
                on_status(f"AI error (aborted): {err}")
            else:
                on_status(f"AI ã‚¨ãƒ©ãƒ¼ï¼ˆä¸­æ­¢ï¼‰: {err}")
            return {"errorHandling": "abort"}

    return _on_error_occurred


_LAST_RUN_DEBUG_LOCK = threading.Lock()
_LAST_RUN_DEBUG: dict[str, Any] | None = None


def _set_last_run_debug(run_debug: dict[str, Any]) -> None:
    global _LAST_RUN_DEBUG
    with _LAST_RUN_DEBUG_LOCK:
        _LAST_RUN_DEBUG = run_debug


def get_last_run_debug() -> dict[str, Any] | None:
    """ç›´è¿‘ã® Copilot SDK å®Ÿè¡Œã®è¦³æ¸¬æƒ…å ±ï¼ˆãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ç­‰ï¼‰ã‚’è¿”ã™ã€‚

    ãƒ¬ãƒãƒ¼ãƒˆæœ¬æ–‡ã«å‡ºã™ç”¨é€”ã§ã¯ãªãã€GUIãƒ­ã‚°/ç›£æŸ»ç”¨ã® input JSON ã«æ·»ä»˜ã™ã‚‹æƒ³å®šã€‚
    """
    with _LAST_RUN_DEBUG_LOCK:
        return dict(_LAST_RUN_DEBUG) if _LAST_RUN_DEBUG else None

# ============================================================
# ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆç®¡ç†
# ============================================================
def list_templates(report_type: str) -> list[dict[str, Any]]:
    """æŒ‡å®šãƒ¬ãƒãƒ¼ãƒˆç¨®åˆ¥ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆä¸€è¦§ã‚’è¿”ã™ã€‚"""
    ensure_user_dirs()

    # user â†’ bundled ã®é †ã§é›†ã‚ã€åŒåãƒ•ã‚¡ã‚¤ãƒ«ã¯ user ã‚’å„ªå…ˆ
    seen: set[str] = set()
    templates: list[dict[str, Any]] = []

    for base in template_search_dirs():
        if not base.exists():
            continue
        for f in sorted(base.glob(f"{report_type}-*.json")):
            key = f.name.lower()
            if key in seen:
                continue
            try:
                data = json.loads(f.read_text(encoding="utf-8"))
                data["_path"] = str(f)
                templates.append(data)
                seen.add(key)
            except (json.JSONDecodeError, OSError):
                pass

    return templates


def load_template(path: str) -> dict[str, Any]:
    """ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆJSONã‚’èª­ã¿è¾¼ã‚€ã€‚"""
    return json.loads(Path(path).read_text(encoding="utf-8"))


def save_template(path: str, data: dict[str, Any]) -> None:
    """ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆJSONã‚’ä¿å­˜ã™ã‚‹ã€‚"""
    p = Path(path)
    p.parent.mkdir(parents=True, exist_ok=True)
    p.write_text(json.dumps(data, indent=2, ensure_ascii=False), encoding="utf-8")


def build_template_instruction(template: dict[str, Any], custom_instruction: str = "") -> str:
    """ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆè¨­å®šã‹ã‚‰AIå‘ã‘ã®æŒ‡ç¤ºãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰ã™ã‚‹ã€‚"""
    sections = template.get("sections", {})
    options = template.get("options", {})

    lang = get_language()

    def _desc(v: dict[str, Any]) -> str:
        if lang == "en":
            return str(v.get("description_en") or v.get("description") or "")
        return str(v.get("description") or v.get("description_en") or "")

    def _label(v: dict[str, Any]) -> str:
        if lang == "en":
            return str(v.get("label_en") or v.get("label") or "")
        return str(v.get("label") or v.get("label_en") or "")

    enabled = [f"- {_label(v)}: {_desc(v)}"
               for _k, v in sections.items() if v.get("enabled")]
    disabled = [f"- {_label(v)}" for _k, v in sections.items() if not v.get("enabled")]

    lines = []
    if lang == "en":
        lines.append("## Report Structure Instructions")
    else:
        lines.append("## ãƒ¬ãƒãƒ¼ãƒˆæ§‹æˆæŒ‡ç¤º")
    lines.append("")
    lines.append(
        "### Included sections (must output):" if lang == "en"
        else "### å«ã‚ã‚‹ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆå¿…ãšå‡ºåŠ›ã™ã‚‹ã“ã¨ï¼‰:"
    )
    lines.extend(enabled)
    lines.append("")
    if disabled:
        lines.append(
            "### Excluded sections (do NOT output):" if lang == "en"
            else "### å«ã‚ãªã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆå‡ºåŠ›ã—ãªã„ã“ã¨ï¼‰:"
        )
        lines.extend(disabled)
        lines.append("")

    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³
    opt_lines = []
    if options.get("show_resource_ids"):
        opt_lines.append("- Show full Resource IDs" if lang == "en" else "- ãƒªã‚½ãƒ¼ã‚¹IDã‚’ãƒ•ãƒ«è¡¨ç¤ºã™ã‚‹")
    else:
        opt_lines.append(
            "- Omit Resource IDs; show resource names only" if lang == "en"
            else "- ãƒªã‚½ãƒ¼ã‚¹IDã¯çœç•¥ã—ã€ãƒªã‚½ãƒ¼ã‚¹åã®ã¿è¡¨ç¤º"
        )
    if options.get("show_mermaid_charts"):
        opt_lines.append("- Include Mermaid charts" if lang == "en" else "- Mermaid ãƒãƒ£ãƒ¼ãƒˆã‚’å«ã‚ã‚‹")
    else:
        opt_lines.append("- Do not include Mermaid charts" if lang == "en" else "- Mermaid ãƒãƒ£ãƒ¼ãƒˆã¯å«ã‚ãªã„")
    if options.get("include_remediation"):
        opt_lines.append("- Include remediation steps" if lang == "en" else "- ä¿®å¾©æ‰‹é †ã‚’å«ã‚ã‚‹")
    if options.get("redact_subscription"):
        opt_lines.append(
            "- Redact subscription IDs (e.g., xxxxxxxx-xxxx-...)" if lang == "en"
            else "- ã‚µãƒ–ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³IDã¯ãƒã‚¹ã‚¯ã™ã‚‹ï¼ˆä¾‹: xxxxxxxx-xxxx-...ï¼‰"
        )
    max_items = options.get("max_detail_items", 10)
    opt_lines.append(
        f"- Limit detail items to max {max_items}" if lang == "en"
        else f"- è©³ç´°é …ç›®ã¯æœ€å¤§ {max_items} ä»¶ã¾ã§"
    )
    currency = options.get("currency_symbol", "")
    if currency:
        opt_lines.append(f"- Currency symbol: {currency}" if lang == "en" else f"- é€šè²¨è¨˜å·: {currency}")

    if opt_lines:
        lines.append("### Output options:" if lang == "en" else "### å‡ºåŠ›ã‚ªãƒ—ã‚·ãƒ§ãƒ³:")
        lines.extend(opt_lines)
        lines.append("")

    # ã‚«ã‚¹ã‚¿ãƒ æŒ‡ç¤º
    if custom_instruction.strip():
        lines.append("### Additional user instructions:" if lang == "en" else "### ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è¿½åŠ æŒ‡ç¤º:")
        lines.append(custom_instruction.strip())
        lines.append("")

    return "\n".join(lines)


# ============================================================
# å®šæ•°
# ============================================================

# ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ãƒ¢ãƒ‡ãƒ«IDã€‚SDK ã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ä¸€è¦§ã‚’å–å¾—ã§ããªã„å ´åˆã«ä½¿ç”¨ã™ã‚‹ã€‚
# å®Ÿè¡Œæ™‚ã®å„ªå…ˆé¸æŠã¯ choose_default_model_id() ã§ claude-sonnet æœ€æ–°ç‰ˆã‚’å„ªå…ˆã™ã‚‹ã€‚
MODEL = "gpt-4.1"
MAX_RETRY = 2
RETRY_BACKOFF = 2.0
SEND_TIMEOUT = 180  # secï¼ˆMCP ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—åˆ†ã‚’è€ƒæ…®ã—ã¦å»¶é•·ï¼‰

# å›³ï¼ˆdraw.io XMLï¼‰ã®ç”Ÿæˆã¯æ™‚é–“ãŒã‹ã‹ã‚Šã‚„ã™ã„ã®ã§ã€åˆ¥æ ã§é•·ã‚ã«å¾…ã¤ã€‚
DRAWIO_SEND_TIMEOUT = 60 * 60  # 60 min
REPORT_SEND_TIMEOUT = 600  # 10 min â€” MCP ãƒ„ãƒ¼ãƒ«åˆ©ç”¨ã‚’è€ƒæ…®
HEARTBEAT_INTERVAL = 5 * 60  # 5 min


def choose_default_model_id(model_ids: list[str]) -> str:
    """ãƒ¢ãƒ‡ãƒ«IDä¸€è¦§ã‹ã‚‰æ—¢å®šãƒ¢ãƒ‡ãƒ«ã‚’é¸ã¶ã€‚

    å„ªå…ˆé †ä½:
      1) claude-sonnet ã®æœ€æ–°ï¼ˆx.y ã‚’æ•°å€¤æ¯”è¼ƒï¼‰
      2) gpt-4.1
      3) å…ˆé ­
    """

    def _sonnet_ver(mid: str) -> tuple[int, int] | None:
        m = re.match(r"^claude-sonnet-(\d+)(?:\.(\d+))?$", mid)
        if not m:
            return None
        major = int(m.group(1))
        minor = int(m.group(2) or 0)
        return (major, minor)

    sonnets: list[tuple[tuple[int, int], str]] = []
    for mid in model_ids:
        v = _sonnet_ver(mid)
        if v:
            sonnets.append((v, mid))
    if sonnets:
        sonnets.sort(key=lambda x: x[0], reverse=True)
        return sonnets[0][1]

    if "gpt-4.1" in model_ids:
        return "gpt-4.1"

    return model_ids[0] if model_ids else MODEL


async def _list_model_ids_async(client: Any) -> list[str]:
    """Copilot SDK ã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«IDã‚’å–å¾—ã™ã‚‹ã€‚"""
    models = await client.list_models()
    ids: list[str] = []
    for m in models:
        mid = getattr(m, "id", None)
        if isinstance(mid, str) and mid.strip():
            ids.append(mid.strip())
    return ids

# Microsoft Docs MCP ã‚µãƒ¼ãƒãƒ¼è¨­å®š
# learn.microsoft.com/api/mcp ã‚’ HTTP MCP ã¨ã—ã¦ SDK ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«æ¥ç¶š
MCP_MICROSOFT_DOCS: dict[str, Any] = {
    "type": "http",
    "url": "https://learn.microsoft.com/api/mcp",
    "tools": ["*"],
}


# ============================================================
# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆè¨€èªå¯¾å¿œï¼‰
# ============================================================


def _system_prompt_review() -> str:
    """ãƒªã‚½ãƒ¼ã‚¹ãƒ¬ãƒ“ãƒ¥ãƒ¼ç”¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆè¨€èªå¯¾å¿œï¼‰ã€‚"""
    if get_language() == "en":
        return """\
You are an Azure infrastructure review expert.
The user will provide a list of Azure resources obtained via Azure Resource Graph.

Do not mention internal tools, tool access, or any tool errors in your response.

Review from the following perspectives and summarize concisely:

1. **Architecture Overview** â€” Infer the system purpose in 2-3 lines
2. **Resource Configuration** â€” Redundancy, HA setup, missing resources
3. **Security** â€” Presence of NSG, Key Vault, Private Endpoint
4. **Cost Optimization** â€” Seemingly unnecessary resources (e.g. duplicate NetworkWatcher)
5. **Diagram Hints** â€” Grouping suggestions

Respond in Markdown, keep the total under 500 words.
"""
    return """\
ã‚ãªãŸã¯ Azure ã‚¤ãƒ³ãƒ•ãƒ©ã‚¹ãƒˆãƒ©ã‚¯ãƒãƒ£ã®ãƒ¬ãƒ“ãƒ¥ãƒ¼å°‚é–€å®¶ã§ã™ã€‚
ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ Azure Resource Graph ã§å–å¾—ã—ãŸãƒªã‚½ãƒ¼ã‚¹ä¸€è¦§ãŒæä¾›ã•ã‚Œã¾ã™ã€‚

å›ç­”ã§ã¯ã€å†…éƒ¨ãƒ„ãƒ¼ãƒ«ã®æœ‰ç„¡ãƒ»ã‚¢ã‚¯ã‚»ã‚¹å¯å¦ãƒ»ãƒ„ãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼ç­‰ã«ã¯ä¸€åˆ‡è§¦ã‚Œãªã„ã§ãã ã•ã„ã€‚

ä»¥ä¸‹ã®è¦³ç‚¹ã§ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã€æ—¥æœ¬èªã§ç°¡æ½”ã«ã¾ã¨ã‚ã¦ãã ã•ã„:

1. **æ§‹æˆæ¦‚è¦** â€” ä½•ã®ã‚·ã‚¹ãƒ†ãƒ ã‹æ¨æ¸¬ã—ã€2-3è¡Œã§èª¬æ˜
2. **ãƒªã‚½ãƒ¼ã‚¹æ§‹æˆã®å¦¥å½“æ€§** â€” å†—é•·æ€§ãƒ»HAæ§‹æˆã®æœ‰ç„¡ã€è¶³ã‚Šãªã„ãƒªã‚½ãƒ¼ã‚¹ã®æŒ‡æ‘˜
3. **ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£** â€” NSG, Key Vault, Private Endpoint ã®æœ‰ç„¡
4. **ã‚³ã‚¹ãƒˆæœ€é©åŒ–** â€” ä¸è¦ã«è¦‹ãˆã‚‹ãƒªã‚½ãƒ¼ã‚¹ï¼ˆNetworkWatcher ã®é‡è¤‡ç­‰ï¼‰
5. **å›³ã«ã™ã‚‹éš›ã®ãƒ’ãƒ³ãƒˆ** â€” ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã®ææ¡ˆ

å›ç­”ã¯ Markdown ã§ã€å…¨ä½“ 500æ–‡å­—ä»¥å†…ã«åã‚ã¦ãã ã•ã„ã€‚
"""


def _caf_security_guidance() -> str:
    """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ï¼ˆè¨€èªå¯¾å¿œï¼‰ã€‚"""
    if get_language() == "en":
        return """
## Compliance Frameworks

Recommendations must be based on these Microsoft official frameworks:
- **Cloud Adoption Framework (CAF)** â€” Security Baseline
- **Well-Architected Framework (WAF)** â€” Security Pillar
- **Azure Security Benchmark v3 (ASB)**
- **Microsoft Defender for Cloud** recommendations

## Environment-Specific Analysis

Read the provided resource list and security data carefully, and point out issues specific to THIS environment:
- Reference actual resource names and types in your comments
- Write "In this environment, X is Y, therefore Z should be done" â€” not generic advice
- Identify VMs without NSG, exposed Public IPs, unused Key Vault, missing Private Endpoints by concrete resource name
- If Secure Score is low, specify what improvements would raise the score

## Documentation references

Use only the reference URLs provided in the prompt (if present).
If no references are provided, proceed with best-effort recommendations without stating tool limitations.

## Output Structure (follow this structure)

1. **Secure Score Summary** â€” If score data is available, show:
   - Current score / Max score
   - Evaluation: 80-100 = Excellent (green), 60-79 = Needs improvement (yellow), 0-59 = Urgent (red)
   - Score improvement opportunities (what specific controls would raise the score)

2. **Recommendations Summary Table** â€” Classify and count by severity:
   | Severity | Count | Description |
   With Critical / High / Medium / Low categories.

3. **Critical & High Severity Findings** â€” For each:
   - Affected resource(s) by name
   - Impact description
   - Remediation steps (actionable commands or portal paths)
   - Reference: [Framework name](URL)

4. **Compliance Posture** â€” If compliance data available:
   | Standard | Passed | Failed | Rate |

5. **Prioritized Action Plan** â€” Separate into:
   - **Immediate (Today)**: Critical security gaps
   - **This Week**: High-priority improvements
   - **This Month**: Medium-priority hardening
   Each item as a checkbox task.

6. **What's Working Well** â€” Acknowledge good security practices found.

## Output Rules
- Classify severity as Critical / High / Medium / Low
- Attach official documentation in the format "Reference: [CAF Security Baseline](URL)" to each recommendation
- Do not comment on resource types that do not exist in this environment

## Data fidelity (IMPORTANT)

- Use ONLY facts present in the provided JSON blocks and resource list.
- Do NOT invent resource names, counts, plan tiers, scores, or settings.
- If a value is missing or null, explicitly write "Unknown" and propose how to verify.
- If you cite references, include the actual URL inline (do not use empty footnotes).

## Tone (customer-aligned)

- Start by acknowledging what's already done well in this environment (if any).
- Use constructive, supportive wording (avoid blaming language).
- When recommending changes, present options and trade-offs (cost, effort, risk).
- Prefer actionable next steps: who should do what, and what to validate.
- If business context is unclear, state assumptions explicitly and keep them reasonable.
"""
    return """
## æº–æ‹ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

æ¨å¥¨äº‹é …ã¯ä»¥ä¸‹ã® Microsoft å…¬å¼ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«åŸºã¥ãã“ã¨:
- **Cloud Adoption Framework (CAF)** â€” ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
- **Well-Architected Framework (WAF)** â€” Security Pillar
- **Azure Security Benchmark v3 (ASB)**
- **Microsoft Defender for Cloud** æ¨å¥¨äº‹é …

## ç’°å¢ƒå›ºæœ‰ã®åˆ†ææŒ‡ç¤º

æä¾›ã•ã‚ŒãŸãƒªã‚½ãƒ¼ã‚¹ä¸€è¦§ã¨ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ‡ãƒ¼ã‚¿ã‚’ã‚ˆãèª­ã¿ã€ã“ã®ç’°å¢ƒå›ºæœ‰ã®å•é¡Œã‚’æŒ‡æ‘˜ã™ã‚‹ã“ã¨:
- å®Ÿéš›ã«å­˜åœ¨ã™ã‚‹ãƒªã‚½ãƒ¼ã‚¹åãƒ»ã‚¿ã‚¤ãƒ—ã‚’å…·ä½“çš„ã«æŒ™ã’ã¦ã‚³ãƒ¡ãƒ³ãƒˆã™ã‚‹
- ã€Œä¸€èˆ¬è«–ã€ã§ãªãã€Œã“ã®ç’°å¢ƒã§ã¯â—‹â—‹ãŒâ–³â–³ã ã‹ã‚‰â–¡â–¡ã™ã¹ãã€ã¨æ›¸ã
- NSGæœªè¨­å®šã® VMã€Public IP éœ²å‡ºã€Key Vault æœªä½¿ç”¨ã€Private Endpoint æœªæ§‹æˆãªã©ã‚’å…·ä½“çš„ãƒªã‚½ãƒ¼ã‚¹åã§æŒ‡æ‘˜
- ã‚»ã‚­ãƒ¥ã‚¢ã‚¹ã‚³ã‚¢ãŒä½ã„å ´åˆã¯ã€å…·ä½“çš„ã«ä½•ã‚’æ”¹å–„ã™ã‚Œã°ã‚¹ã‚³ã‚¢ãŒä¸ŠãŒã‚‹ã‹è¨€åŠ

## å‚è€ƒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã«æç¤ºã•ã‚ŒãŸå‚ç…§URLï¼ˆå­˜åœ¨ã™ã‚Œã°ï¼‰ã ã‘ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚
å‚ç…§ãŒç„¡ã„å ´åˆã§ã‚‚ã€ãƒ„ãƒ¼ãƒ«åˆ¶ç´„ãªã©ã®å†…éƒ¨äº‹æƒ…ã¯æ›¸ã‹ãšã«ã€ãƒ™ã‚¹ãƒˆã‚¨ãƒ•ã‚©ãƒ¼ãƒˆã§æ¨å¥¨ã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚

## å‡ºåŠ›æ§‹æˆï¼ˆã“ã®æ§‹æˆã«å¾“ã†ã“ã¨ï¼‰

1. **ã‚»ã‚­ãƒ¥ã‚¢ã‚¹ã‚³ã‚¢æ¦‚æ³** â€” ã‚¹ã‚³ã‚¢ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°:
   - ç¾åœ¨ã®ã‚¹ã‚³ã‚¢ / æœ€å¤§ã‚¹ã‚³ã‚¢
   - è©•ä¾¡: 80-100 = ğŸŸ¢ å„ªè‰¯ã€60-79 = ğŸŸ¡ è¦æ”¹å–„ã€0-59 = ğŸ”´ è¦ç·Šæ€¥å¯¾å¿œ
   - ã‚¹ã‚³ã‚¢æ”¹å–„ã®æ©Ÿä¼šï¼ˆå…·ä½“çš„ã«ã©ã®ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ã‚’æ”¹å–„ã™ã‚Œã°ä¸ŠãŒã‚‹ã‹ï¼‰

2. **æ¨å¥¨äº‹é …ã‚µãƒãƒªãƒ¼è¡¨** â€” æ·±åˆ»åº¦ã§åˆ†é¡ãƒ»ä»¶æ•°è¡¨ç¤º:
   | æ·±åˆ»åº¦ | ä»¶æ•° | æ¦‚è¦ |
   Critical / High / Medium / Low ã§åˆ†é¡ã€‚

3. **Critical & High ã®è©³ç´°** â€” å„é …ç›®ã«:
   - å¯¾è±¡ãƒªã‚½ãƒ¼ã‚¹å
   - å½±éŸ¿ã®èª¬æ˜
   - ä¿®å¾©æ‰‹é †ï¼ˆå®Ÿè¡Œå¯èƒ½ãªã‚³ãƒãƒ³ãƒ‰ã‚„ãƒãƒ¼ã‚¿ãƒ«ãƒ‘ã‚¹ï¼‰
   - æ ¹æ‹ : [ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯å](URL)

4. **ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ãƒã‚¹ãƒãƒ£ãƒ¼** â€” ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°:
   | æ¨™æº– | æº–æ‹  | éæº–æ‹  | æº–æ‹ ç‡ |

5. **å„ªå…ˆåº¦åˆ¥ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ—ãƒ©ãƒ³** â€” ä»¥ä¸‹ã®3æ®µã«åˆ†é›¢:
   - **å³åº§ã«å¯¾å¿œï¼ˆå½“æ—¥ï¼‰**: Critical ãªã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã‚®ãƒ£ãƒƒãƒ—
   - **ä»Šé€±ä¸­ã«å¯¾å¿œ**: High å„ªå…ˆåº¦ã®æ”¹å–„
   - **ä»Šæœˆä¸­ã«å¯¾å¿œ**: Medium å„ªå…ˆåº¦ã®ãƒãƒ¼ãƒ‰ãƒ‹ãƒ³ã‚°
   å„é …ç›®ã‚’ãƒã‚§ãƒƒã‚¯ãƒœãƒƒã‚¯ã‚¹å½¢å¼ã§è¨˜è¼‰ã€‚

6. **ã“ã®ç’°å¢ƒã§è‰¯ãã§ãã¦ã„ã‚‹ç‚¹** â€” æ—¢ã«ã‚ã‚‹ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£å¯¾ç­–ã‚’è©•ä¾¡ã€‚

## å‡ºåŠ›ãƒ«ãƒ¼ãƒ«
- æ·±åˆ»åº¦ã¯ Critical / High / Medium / Low ã§åˆ†é¡
- å„æ¨å¥¨äº‹é …ã«ã€Œæ ¹æ‹ : [CAF Security Baseline](URL)ã€ã®å½¢å¼ã§å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä»˜ä¸
- ç’°å¢ƒã«å­˜åœ¨ã—ãªã„ãƒªã‚½ãƒ¼ã‚¹ã«ã¤ã„ã¦ã®æŒ‡æ‘˜ã¯ã—ãªã„

## ãƒ‡ãƒ¼ã‚¿å¿ å®Ÿæ€§ï¼ˆé‡è¦ï¼‰

- äº‹å®Ÿã¨ã—ã¦æ›¸ã„ã¦ã‚ˆã„ã®ã¯ã€æç¤ºã•ã‚ŒãŸ JSON ãƒ–ãƒ­ãƒƒã‚¯ã¨ãƒªã‚½ãƒ¼ã‚¹ä¸€è¦§ã«å«ã¾ã‚Œã‚‹å†…å®¹ã®ã¿ã€‚
- ãƒªã‚½ãƒ¼ã‚¹åãƒ»ä»¶æ•°ãƒ»Defender ã® tierãƒ»ã‚¹ã‚³ã‚¢ãƒ»è¨­å®šå€¤ãªã©ã‚’æ¨æ¸¬ã§ã€Œæ–­å®šã€ã—ãªã„ã€‚
- å€¤ãŒæ¬ ã‘ã¦ã„ã‚‹/å–å¾—ã§ãã¦ã„ãªã„å ´åˆã¯ã€Œä¸æ˜ã€ã¨æ˜è¨˜ã—ã€ç¢ºèªæ‰‹é †ã‚’ææ¡ˆã™ã‚‹ã€‚
- å‚ç…§ã‚’ä»˜ã‘ã‚‹å ´åˆã¯ URL ã‚’æœ¬æ–‡ã«å«ã‚ã‚‹ï¼ˆURL ãªã—è„šæ³¨ã ã‘ã€ã¯ä¸å¯ï¼‰ã€‚

## ãƒˆãƒ¼ãƒ³ï¼ˆé¡§å®¢ã«å¯„ã‚Šæ·»ã†ï¼‰

- ã¾ãšã€ã“ã®ç’°å¢ƒã§ã€Œã§ãã¦ã„ã‚‹ç‚¹ã€ã‚’çŸ­ãèªã‚ã‚‹ï¼ˆè©²å½“ãŒã‚ã‚Œã°ï¼‰ã€‚
- è²¬ã‚ã‚‹è¡¨ç¾ã¯é¿ã‘ã€å»ºè¨­çš„ãƒ»æ”¯æ´çš„ãªè¨€ã„å›ã—ã«ã™ã‚‹ã€‚
- æ¨å¥¨ã¯ä¸€æŠã«ã›ãšã€ã‚³ã‚¹ãƒˆ/å·¥æ•°/ãƒªã‚¹ã‚¯ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•ã‚’ç¤ºã™ã€‚
- ã€Œæ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã€ã‚’å…·ä½“çš„ã«ï¼ˆèª°ãŒãƒ»ä½•ã‚’ãƒ»ä½•ã‚’ç¢ºèªã™ã‚‹ã‹ï¼‰ã€‚
- é¡§å®¢ã®ç›®çš„ãŒä¸æ˜ãªå ´åˆã¯ã€å‰æã‚’æ˜è¨˜ã—ã¦æ§ãˆã‚ã«æ¨æ¸¬ã™ã‚‹ã€‚
"""


def _system_prompt_security_base() -> str:
    """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¬ãƒãƒ¼ãƒˆç”¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆè¨€èªå¯¾å¿œï¼‰ã€‚"""
    guidance = _caf_security_guidance()
    if get_language() == "en":
        return f"""\
You are an Azure security audit expert.
You will be provided with Azure Security Center / Microsoft Defender for Cloud data and the actual Azure environment resource list.

Do not mention internal tools, tool access, or any tool errors in your output.

Your report comments must be **specific findings for this particular environment** based on the provided data.
Prioritize specificity: "Resource X in this environment is Y, therefore Z should be done" â€” not generic advice.
Output in English Markdown format, using tables and lists for readability.
{guidance}
"""
    return f"""\
ã‚ãªãŸã¯ Azure ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ç›£æŸ»ã®å°‚é–€å®¶ã§ã™ã€‚
Azure Security Center / Microsoft Defender for Cloud ã®ãƒ‡ãƒ¼ã‚¿ã¨ã€å®Ÿéš›ã® Azure ç’°å¢ƒã®ãƒªã‚½ãƒ¼ã‚¹ä¸€è¦§ãŒæä¾›ã•ã‚Œã¾ã™ã€‚

å‡ºåŠ›ã§ã¯ã€å†…éƒ¨ãƒ„ãƒ¼ãƒ«ã®æœ‰ç„¡ãƒ»ã‚¢ã‚¯ã‚»ã‚¹å¯å¦ãƒ»ãƒ„ãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼ç­‰ã«ã¯ä¸€åˆ‡è§¦ã‚Œãªã„ã§ãã ã•ã„ã€‚

ãƒ¬ãƒãƒ¼ãƒˆã®ã‚³ãƒ¡ãƒ³ãƒˆã¯ã€æä¾›ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è§£ã„ãŸä¸Šã§ã€Œã“ã®ç’°å¢ƒå›ºæœ‰ã®å…·ä½“çš„ãªæŒ‡æ‘˜ã€ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚
ä¸€èˆ¬è«–ã§ã¯ãªãã€ã€Œã“ã®ç’°å¢ƒã® â—‹â—‹ ã¯ â–³â–³ ã ã‹ã‚‰ â–¡â–¡ ã™ã¹ãã€ã¨ã„ã†å…·ä½“æ€§ã‚’æœ€å„ªå…ˆã—ã¦ãã ã•ã„ã€‚
æ—¥æœ¬èªã® Markdown å½¢å¼ã§ã€è¡¨ã‚„ãƒªã‚¹ãƒˆã‚’æ´»ç”¨ã—ã¦èª­ã¿ã‚„ã™ãã€‚
{guidance}
"""


def _caf_cost_guidance() -> str:
    """ã‚³ã‚¹ãƒˆã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ï¼ˆè¨€èªå¯¾å¿œï¼‰ã€‚"""
    if get_language() == "en":
        return """
## Compliance Frameworks

Recommendations must be based on these Microsoft official frameworks:
- **Cloud Adoption Framework (CAF)** â€” Cost Management best practices
- **Well-Architected Framework (WAF)** â€” Cost Optimization Pillar / Checklist
- **FinOps Framework** â€” Cloud cost optimization practices
- **Azure Advisor** â€” Cost recommendations

## Environment-Specific Analysis

Read the provided cost data and resource list carefully, and point out issues specific to THIS environment:
- Name top-cost resources explicitly, mention SKU downgrade or reservation purchase opportunities
- For resources with Advisor recommendations, provide specific savings amounts and remediation steps
- Write "Resource X in this environment costs $Y/month; doing Z would save $W" â€” not generic advice
- Identify unused or underutilized resources by name and recommend stopping/deleting
- If resources lack tags, flag this from a FinOps cost allocation perspective

## Documentation references

Use only the reference URLs provided in the prompt (if present).
If no references are provided, proceed with best-effort recommendations without stating tool limitations.

## Output Structure (follow this structure)

1. **Cost Summary** â€” Show totals with trend indicator:
   - Total cost (MonthToDate)
   - Month-over-month change (% and absolute)
   - Trend: increasing / decreasing / stable

2. **Budget Consumption** â€” If budget data available:
   - Budget amount, consumed, remaining, percentage bar
   - Warning if > 80% consumed before month end

3. **Cost by Service** â€” Top services table with cost, percentage, bar visualization:
   | Service | Cost | % | Bar |

4. **Cost by Resource Group** â€” Table with costs sorted descending.

5. **Top 10 Resources** â€” Include resource name, type, RG, cost, owner/env tags.
   Flag resources missing cost allocation tags.

6. **Cost Anomaly Detection** â€” Resources with >50% cost increase vs prior period.
   | Resource | Prior Cost | Current Cost | Increase % | Possible Cause |

7. **Idle/Underutilized Resources** â€” Detect and list:
   - VMs with avg CPU < 5% (past 14 days)
   - Unattached disks, unused Public IPs
   - Storage accounts with no recent access
   Include estimated monthly savings for each.

8. **Optimization Recommendations** â€” Separate into:
   - **Quick Wins** (low effort, immediate savings): e.g., stop idle VMs, delete unattached disks
   - **Strategic Changes** (higher effort, larger savings): e.g., reserved instances, right-sizing
   Each with estimated savings amount and confidence level.

9. **Tag Governance** â€” If tag data available:
   - Tag coverage rate
   - Cost by department / environment / project tags
   - Untagged high-cost resources list

## Output Rules
- Attach official documentation in the format "Reference: [WAF Cost Optimization](URL)" to each recommendation
- Include currency symbols with amounts, use tables for readability
- Do not comment on resource types that do not exist in this environment

## Data fidelity (IMPORTANT)

- Use ONLY facts present in the provided JSON blocks and resource list.
- Do NOT invent resource names, costs, savings amounts, SKUs, or tags.
- If a value is missing or null, explicitly write "Unknown" and propose how to verify.
- If you cite references, include the actual URL inline (do not use empty footnotes).

## Tone (customer-aligned)

- Highlight good practices found (e.g., tags, reservations, budgets) before pointing out gaps.
- Be sensitive to operational constraints (e.g., production workloads, compliance).
- Separate **Quick wins** (low effort) and **Strategic changes** (higher effort).
- Avoid recommending deletion when uncertainty is high; suggest validation steps first.
"""
    return """
## æº–æ‹ ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯

æ¨å¥¨äº‹é …ã¯ä»¥ä¸‹ã® Microsoft å…¬å¼ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯ã«åŸºã¥ãã“ã¨:
- **Cloud Adoption Framework (CAF)** â€” ã‚³ã‚¹ãƒˆç®¡ç†ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹
- **Well-Architected Framework (WAF)** â€” Cost Optimization Pillar / ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆ
- **FinOps Framework** â€” ã‚¯ãƒ©ã‚¦ãƒ‰ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã®å®Ÿè·µ
- **Azure Advisor** â€” ã‚³ã‚¹ãƒˆæ¨å¥¨äº‹é …

## ç’°å¢ƒå›ºæœ‰ã®åˆ†ææŒ‡ç¤º

æä¾›ã•ã‚ŒãŸã‚³ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ãƒªã‚½ãƒ¼ã‚¹ä¸€è¦§ã‚’ã‚ˆãèª­ã¿ã€ã“ã®ç’°å¢ƒå›ºæœ‰ã®å•é¡Œã‚’æŒ‡æ‘˜ã™ã‚‹ã“ã¨:
- ã‚³ã‚¹ãƒˆä¸Šä½ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’å…·ä½“åã§æŒ™ã’ã€SKU ãƒ€ã‚¦ãƒ³ã‚°ãƒ¬ãƒ¼ãƒ‰ã‚„äºˆç´„è³¼å…¥ã®å¯èƒ½æ€§ã‚’è¨€åŠ
- Advisor æ¨å¥¨ãŒã‚ã‚‹ãƒªã‚½ãƒ¼ã‚¹ã¯å…·ä½“çš„ãªå‰Šæ¸›é¡ã¨å¯¾å¿œæ–¹æ³•ã‚’è¨˜è¼‰
- ã€Œä¸€èˆ¬è«–ã€ã§ã¯ãªãã€Œã“ã®ç’°å¢ƒã® â—‹â—‹ ã¯ æœˆé¡ Xå†† ã‹ã‹ã£ã¦ãŠã‚Šã€â–³â–³ ã™ã‚Œã° Y å†† å‰Šæ¸›å¯èƒ½ã€ã¨æ›¸ã
- æœªä½¿ç”¨ãƒ»ä½ç¨¼åƒãƒªã‚½ãƒ¼ã‚¹ã¯å…·ä½“åã‚’æŒ™ã’ã¦åœæ­¢ãƒ»å‰Šé™¤ã‚’æ¨å¥¨
- ã‚¿ã‚°æœªè¨­å®šã®ãƒªã‚½ãƒ¼ã‚¹ãŒã‚ã‚Œã°ã€FinOps ã®ã€Œã‚³ã‚¹ãƒˆé…åˆ†ã€ã®è¦³ç‚¹ã§æŒ‡æ‘˜

## å‚è€ƒãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã«æç¤ºã•ã‚ŒãŸå‚ç…§URLï¼ˆå­˜åœ¨ã™ã‚Œã°ï¼‰ã ã‘ã‚’ä½¿ã£ã¦ãã ã•ã„ã€‚
å‚ç…§ãŒç„¡ã„å ´åˆã§ã‚‚ã€ãƒ„ãƒ¼ãƒ«åˆ¶ç´„ãªã©ã®å†…éƒ¨äº‹æƒ…ã¯æ›¸ã‹ãšã«ã€ãƒ™ã‚¹ãƒˆã‚¨ãƒ•ã‚©ãƒ¼ãƒˆã§æ¨å¥¨ã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚

## å‡ºåŠ›æ§‹æˆï¼ˆã“ã®æ§‹æˆã«å¾“ã†ã“ã¨ï¼‰

1. **ã‚³ã‚¹ãƒˆæ¦‚æ³** â€” ãƒˆãƒ¬ãƒ³ãƒ‰ä»˜ãã®åˆè¨ˆè¡¨ç¤º:
   - ç·ã‚³ã‚¹ãƒˆï¼ˆæœˆåˆæ¥ï¼‰
   - å‰æœˆæ¯”ï¼ˆ% ã¨çµ¶å¯¾é¡ï¼‰
   - ãƒˆãƒ¬ãƒ³ãƒ‰: ğŸ“ˆ å¢—åŠ  / ğŸ“‰ æ¸›å°‘ / â¡ï¸ å®‰å®š

2. **äºˆç®—æ¶ˆåŒ–çŠ¶æ³** â€” äºˆç®—ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°:
   - äºˆç®—é¡ã€æ¶ˆåŒ–é¡ã€æ®‹ã‚Šã€ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆ
   - æœˆæœ«å‰ã«80%è¶…æ¶ˆåŒ–ã®å ´åˆã¯è­¦å‘Š

3. **ã‚µãƒ¼ãƒ“ã‚¹åˆ¥ã‚³ã‚¹ãƒˆ** â€” ä¸Šä½ã‚µãƒ¼ãƒ“ã‚¹ã‚’ãƒ†ãƒ¼ãƒ–ãƒ«è¡¨ç¤º:
   | ã‚µãƒ¼ãƒ“ã‚¹ | ã‚³ã‚¹ãƒˆ | å‰²åˆ | ãƒãƒ¼ |

4. **ãƒªã‚½ãƒ¼ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—åˆ¥ã‚³ã‚¹ãƒˆ** â€” ã‚³ã‚¹ãƒˆé™é †ãƒ†ãƒ¼ãƒ–ãƒ«ã€‚

5. **ã‚³ã‚¹ãƒˆä¸Šä½10ãƒªã‚½ãƒ¼ã‚¹** â€” ãƒªã‚½ãƒ¼ã‚¹åã€ç¨®é¡ã€RGã€ã‚³ã‚¹ãƒˆã€owner/envã‚¿ã‚°ã€‚
   ã‚³ã‚¹ãƒˆé…åˆ†ã‚¿ã‚°ãŒæœªè¨­å®šã®ãƒªã‚½ãƒ¼ã‚¹ã¯æŒ‡æ‘˜ã€‚

6. **ã‚³ã‚¹ãƒˆç•°å¸¸æ¤œçŸ¥** â€” å‰æœˆæ¯”50%ä»¥ä¸Šå¢—åŠ ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’æ¤œå‡º:
   | ãƒªã‚½ãƒ¼ã‚¹ | å‰æœˆã‚³ã‚¹ãƒˆ | ä»Šæœˆã‚³ã‚¹ãƒˆ | å¢—åŠ ç‡ | è€ƒãˆã‚‰ã‚Œã‚‹åŸå›  |

7. **æœªä½¿ç”¨ãƒ»ä½ç¨¼åƒãƒªã‚½ãƒ¼ã‚¹** â€” æ¤œå‡ºã—ã¦ä¸€è¦§åŒ–:
   - éå»14æ—¥ã®å¹³å‡CPU 5%æœªæº€ã® VM
   - æœªæ¥ç¶šãƒ‡ã‚£ã‚¹ã‚¯ã€æœªä½¿ç”¨ Public IP
   - ã‚¢ã‚¯ã‚»ã‚¹ã®ãªã„ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸ã‚¢ã‚«ã‚¦ãƒ³ãƒˆ
   å„é …ç›®ã«æ¨å®šæœˆé¡å‰Šæ¸›é¡ã‚’ä»˜è¨˜ã€‚

8. **æœ€é©åŒ–æ¨å¥¨äº‹é …** â€” ä»¥ä¸‹ã®2æ®µã«åˆ†é›¢:
   - **Quick Win**ï¼ˆä½å·¥æ•°ãƒ»å³åŠ¹æ€§ï¼‰: ã‚¢ã‚¤ãƒ‰ãƒ«VMåœæ­¢ã€æœªæ¥ç¶šãƒ‡ã‚£ã‚¹ã‚¯å‰Šé™¤ãªã©
   - **Strategic**ï¼ˆä¸­é•·æœŸãƒ»å¤§ããªå‰Šæ¸›ï¼‰: äºˆç´„ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã€ãƒ©ã‚¤ãƒˆã‚µã‚¤ã‚¸ãƒ³ã‚°ãªã©
   å„é …ç›®ã«æ¨å®šå‰Šæ¸›é¡ã¨ä¿¡é ¼åº¦ã‚’ä»˜è¨˜ã€‚

9. **ã‚¿ã‚°ã‚¬ãƒãƒŠãƒ³ã‚¹** â€” ã‚¿ã‚°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Œã°:
   - ã‚¿ã‚°è¨­å®šç‡
   - éƒ¨é–€/ç’°å¢ƒ/ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåˆ¥ã‚³ã‚¹ãƒˆé›†è¨ˆ
   - ã‚¿ã‚°æœªè¨­å®šã®é«˜ã‚³ã‚¹ãƒˆãƒªã‚½ãƒ¼ã‚¹ä¸€è¦§

## å‡ºåŠ›ãƒ«ãƒ¼ãƒ«
- å„æ¨å¥¨äº‹é …ã«ã€Œæ ¹æ‹ : [WAF Cost Optimization](URL)ã€ã®å½¢å¼ã§å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä»˜ä¸
- é‡‘é¡ã¯é€šè²¨è¨˜å·ä»˜ãã§ã€è¡¨ã‚’æ´»ç”¨ã—ã¦èª­ã¿ã‚„ã™ã
- ç’°å¢ƒã«å­˜åœ¨ã—ãªã„ãƒªã‚½ãƒ¼ã‚¹ã«ã¤ã„ã¦ã®æŒ‡æ‘˜ã¯ã—ãªã„

## ãƒ‡ãƒ¼ã‚¿å¿ å®Ÿæ€§ï¼ˆé‡è¦ï¼‰

- äº‹å®Ÿã¨ã—ã¦æ›¸ã„ã¦ã‚ˆã„ã®ã¯ã€æç¤ºã•ã‚ŒãŸ JSON ãƒ–ãƒ­ãƒƒã‚¯ã¨ãƒªã‚½ãƒ¼ã‚¹ä¸€è¦§ã«å«ã¾ã‚Œã‚‹å†…å®¹ã®ã¿ã€‚
- ãƒªã‚½ãƒ¼ã‚¹åãƒ»ã‚³ã‚¹ãƒˆãƒ»å‰Šæ¸›é¡ãƒ»SKUãƒ»ã‚¿ã‚°ãªã©ã‚’æ¨æ¸¬ã§ã€Œæ–­å®šã€ã—ãªã„ã€‚
- å€¤ãŒæ¬ ã‘ã¦ã„ã‚‹/å–å¾—ã§ãã¦ã„ãªã„å ´åˆã¯ã€Œä¸æ˜ã€ã¨æ˜è¨˜ã—ã€ç¢ºèªæ‰‹é †ã‚’ææ¡ˆã™ã‚‹ã€‚
- å‚ç…§ã‚’ä»˜ã‘ã‚‹å ´åˆã¯ URL ã‚’æœ¬æ–‡ã«å«ã‚ã‚‹ï¼ˆURL ãªã—è„šæ³¨ã ã‘ã€ã¯ä¸å¯ï¼‰ã€‚

## ãƒˆãƒ¼ãƒ³ï¼ˆé¡§å®¢ã«å¯„ã‚Šæ·»ã†ï¼‰

- ã§ãã¦ã„ã‚‹é‹ç”¨ï¼ˆã‚¿ã‚°ã€äºˆç´„ã€äºˆç®—ãªã©ï¼‰ãŒã‚ã‚Œã°å…ˆã«è©•ä¾¡ã™ã‚‹ã€‚
- æœ¬ç•ª/ã‚³ãƒ³ãƒ—ãƒ©ã‚¤ã‚¢ãƒ³ã‚¹ç­‰ã®åˆ¶ç´„ã‚’å‰æã«ã€ç„¡ç†ã®ãªã„ææ¡ˆã«ã™ã‚‹ã€‚
- **Quick win**ï¼ˆä½å·¥æ•°ï¼‰ã¨ **Strategic**ï¼ˆä¸­é•·æœŸï¼‰ã‚’åˆ†ã‘ã¦ææ¡ˆã™ã‚‹ã€‚
- ä¸ç¢ºå®Ÿæ€§ãŒé«˜ã„ã‚‚ã®ã¯å³å‰Šé™¤æ¨å¥¨ã›ãšã€æ¤œè¨¼æ‰‹é †â†’åˆ¤æ–­ã®é †ã«ã™ã‚‹ã€‚
"""


def _system_prompt_cost_base() -> str:
    """ã‚³ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆç”¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆè¨€èªå¯¾å¿œï¼‰ã€‚"""
    guidance = _caf_cost_guidance()
    if get_language() == "en":
        return f"""\
You are an Azure cost optimization expert.
You will be provided with Azure Cost Management data (cost by service / by RG) and the actual Azure environment resource list.

Do not mention internal tools, tool access, or any tool errors in your output.

Your report comments must be **specific findings for this particular environment** based on the provided data.
Prioritize specificity: "Resource X in this environment is Y, therefore Z should be done" â€” not generic advice.
Output in English Markdown format, using tables and lists for readability.
{guidance}
"""
    return f"""\
ã‚ãªãŸã¯ Azure ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã®å°‚é–€å®¶ã§ã™ã€‚
Azure Cost Management ã®ãƒ‡ãƒ¼ã‚¿ï¼ˆã‚µãƒ¼ãƒ“ã‚¹åˆ¥ãƒ»RGåˆ¥ã‚³ã‚¹ãƒˆï¼‰ã¨ã€å®Ÿéš›ã® Azure ç’°å¢ƒã®ãƒªã‚½ãƒ¼ã‚¹ä¸€è¦§ãŒæä¾›ã•ã‚Œã¾ã™ã€‚

å‡ºåŠ›ã§ã¯ã€å†…éƒ¨ãƒ„ãƒ¼ãƒ«ã®æœ‰ç„¡ãƒ»ã‚¢ã‚¯ã‚»ã‚¹å¯å¦ãƒ»ãƒ„ãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼ç­‰ã«ã¯ä¸€åˆ‡è§¦ã‚Œãªã„ã§ãã ã•ã„ã€‚

ãƒ¬ãƒãƒ¼ãƒˆã®ã‚³ãƒ¡ãƒ³ãƒˆã¯ã€æä¾›ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è§£ã„ãŸä¸Šã§ã€Œã“ã®ç’°å¢ƒå›ºæœ‰ã®å…·ä½“çš„ãªæŒ‡æ‘˜ã€ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚
ä¸€èˆ¬è«–ã§ã¯ãªãã€ã€Œã“ã®ç’°å¢ƒã® â—‹â—‹ ã¯ â–³â–³ ã ã‹ã‚‰ â–¡â–¡ ã™ã¹ãã€ã¨ã„ã†å…·ä½“æ€§ã‚’æœ€å„ªå…ˆã—ã¦ãã ã•ã„ã€‚
æ—¥æœ¬èªã® Markdown å½¢å¼ã§ã€è¡¨ã‚„ãƒªã‚¹ãƒˆã‚’æ´»ç”¨ã—ã¦èª­ã¿ã‚„ã™ãã€‚
{guidance}
"""



# ============================================================
# CopilotClient ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å˜ä½ã§å†åˆ©ç”¨ï¼‰
# ============================================================

_cached_client: Any | None = None
_cached_client_started: bool = False

# åŒæ™‚ã«è¤‡æ•°ã® generate/report ãŒèµ°ã£ãŸå ´åˆã§ã‚‚ã€CopilotClient.start() ã‚’
# äºŒé‡å®Ÿè¡Œã—ãªã„ãŸã‚ã®éåŒæœŸãƒ­ãƒƒã‚¯ï¼ˆåŒä¸€ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—å†…ã§ç›´åˆ—åŒ–ã™ã‚‹ï¼‰ã€‚
_client_create_lock: asyncio.Lock | None = None


def _get_client_create_lock() -> asyncio.Lock:
    global _client_create_lock
    if _client_create_lock is None:
        _client_create_lock = asyncio.Lock()
    return _client_create_lock


async def _get_or_create_client(
    on_status: Optional[Callable[[str], None]] = None,
) -> Any:
    """CopilotClient ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã—ã¦è¿”ã™ã€‚

    é€£ç¶šãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆæ™‚ã«æ¯å›æ¥ç¶šâ†’åœæ­¢ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’æ’é™¤ã™ã‚‹ã€‚
    asyncio.Lock ã§ç›´åˆ—åŒ–ã—ã€_bg_lockï¼ˆthreading.Lockï¼‰ã¯ async å¤–ã®
    çŸ­ã„ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå‚ç…§ã«ã®ã¿ä½¿ç”¨ã—ã¦ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã®ãƒ–ãƒ­ãƒƒã‚¯ã‚’é˜²ãã€‚
    """
    global _cached_client, _cached_client_started
    log = on_status or (lambda s: None)

    # é«˜é€Ÿãƒ‘ã‚¹: ãƒ­ãƒƒã‚¯å–å¾—å‰ã«ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆãƒã‚§ãƒƒã‚¯ï¼ˆthreading.Lock ã¯ç¬æ™‚ã«è§£æ”¾ï¼‰
    cached_client: Any | None = None
    with _bg_lock:
        if _cached_client is not None and _cached_client_started:
            cached_client = _cached_client
    if cached_client is not None:
        # NOTE: on_status ã¯ä»»æ„ã®å®Ÿè£…ï¼ˆGUIãƒ­ã‚°ç­‰ï¼‰ã«ãªã‚Šå¾—ã‚‹ãŸã‚ã€ãƒ­ãƒƒã‚¯å¤–ã§å‘¼ã¶ã€‚
        log("Copilot SDK: Reusing cached client" if get_language() == "en" else "Copilot SDK: ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¸ˆã¿ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å†åˆ©ç”¨")
        return cached_client

    lock = _get_client_create_lock()
    async with lock:
        # ãƒ€ãƒ–ãƒ«ãƒã‚§ãƒƒã‚¯: ä¸¦è¡Œã‚¿ã‚¹ã‚¯ãŒå…ˆã«ä½œæˆã—ãŸå ´åˆ
        cached_client = None
        with _bg_lock:
            if _cached_client is not None and _cached_client_started:
                cached_client = _cached_client
        if cached_client is not None:
            log("Copilot SDK: Reusing cached client" if get_language() == "en" else "Copilot SDK: ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¸ˆã¿ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å†åˆ©ç”¨")
            return cached_client

        log("Copilot SDK: Connecting..." if get_language() == "en" else "Copilot SDK ã«æ¥ç¶šä¸­...")
        client_opts: Any = {
            "auto_restart": True,
        }
        cli = copilot_cli_path()
        if cli:
            client_opts["cli_path"] = cli
            log(f"CLI path: {cli}")

        if CopilotClient is None:
            details = _COPILOT_IMPORT_ERROR or "unknown import error"
            log(f"âš  Copilot SDK not available: {details}" if get_language() == "en" else f"âš  Copilot SDK ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {details}")
            # frozen exe ã§ã¯ã€åŒæ¢±ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåãŒ 'copilot' ã¨è¡çªã™ã‚‹ã¨ ImportError ã«ãªã‚Šã‚„ã™ã„ã€‚
            raise RuntimeError(
                f"Copilot SDK is not available.\n"
                f"â†’ Run: uv pip install copilot-sdk\n"
                f"Import error: {details}"
            )

        new_client = CopilotClient(options=client_opts)
        await new_client.start()

        with _bg_lock:
            _cached_client = new_client
            _cached_client_started = True

        log("Copilot SDK: Connected" if get_language() == "en" else "Copilot SDK æ¥ç¶š OK")
        return new_client


async def shutdown_cached_client() -> None:
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã«ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¸ˆã¿ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’åœæ­¢ã€‚"""
    global _cached_client, _cached_client_started
    if _cached_client and _cached_client_started:
        try:
            await _cached_client.stop()
        except Exception:
            pass
        finally:
            _cached_client = None
            _cached_client_started = False


def shutdown_sync() -> None:
    """åŒæœŸç‰ˆã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ï¼ˆtkinter ã® on_close ã‹ã‚‰å‘¼ã¶ç”¨ï¼‰ã€‚"""
    global _bg_loop, _bg_thread
    # 1. CopilotClient ã‚’åœæ­¢
    loop = _bg_loop
    if loop and not loop.is_closed():
        try:
            future = asyncio.run_coroutine_threadsafe(shutdown_cached_client(), loop)
            future.result(timeout=5)
        except Exception:
            pass
        # 2. ã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’åœæ­¢
        loop.call_soon_threadsafe(loop.stop)
    _bg_loop = None
    _bg_thread = None


# ============================================================
# Reviewer ã‚¯ãƒ©ã‚¹
# ============================================================

class AIReviewer:
    """Copilot SDK ã‚’ä½¿ã£ãŸãƒªã‚½ãƒ¼ã‚¹ãƒ¬ãƒ“ãƒ¥ãƒ¼ / ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆã€‚

    Usage::
        reviewer = AIReviewer(on_delta=print)
        result = await reviewer.review(resource_summary_text)
        result = await reviewer.generate(prompt, system_prompt)
    """

    def __init__(
        self,
        on_delta: Optional[Callable[[str], None]] = None,
        on_status: Optional[Callable[[str], None]] = None,
        model_id: str | None = None,
    ) -> None:
        self._on_delta = on_delta or (lambda s: print(s, end="", flush=True))
        self._on_status = on_status or (lambda s: print(f"[reviewer] {s}"))
        self._model_id = model_id

    async def review(self, resource_text: str) -> str | None:
        """ãƒªã‚½ãƒ¼ã‚¹ã‚µãƒãƒªã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã€çµæœãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™ã€‚"""
        if get_language() == "en":
            prompt = (
                "Please review the following Azure resource list:\n\n"
                f"```\n{resource_text}\n```"
            )
        else:
            prompt = (
                "ä»¥ä¸‹ã®Azureãƒªã‚½ãƒ¼ã‚¹ä¸€è¦§ã‚’ãƒ¬ãƒ“ãƒ¥ãƒ¼ã—ã¦ãã ã•ã„:\n\n"
                f"```\n{resource_text}\n```"
            )
        return await self.generate(prompt, _system_prompt_review(), model_id=self._model_id)

    async def generate(
        self,
        prompt: str,
        system_prompt: str,
        *,
        model_id: str | None = None,
        append_language_instruction: bool = True,
        timeout_s: float | None = None,
        heartbeat_s: float | None = None,
    ) -> str | None:
        """æ±ç”¨: ä»»æ„ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã§ç”Ÿæˆã€‚

        SDK æ¨å¥¨ãƒ‘ã‚¿ãƒ¼ãƒ³:
          - session.idle ã‚¤ãƒ™ãƒ³ãƒˆ + asyncio.Event ã§å®Œäº†å¾…ã¡
          - hooks.on_error_occurred ã§ãƒªãƒˆãƒ©ã‚¤åˆ¶å¾¡
          - reasoning_delta å¯¾å¿œ
          - on_pre_tool_use ã§èª­ã¿å–ã‚Šå°‚ç”¨ãƒ„ãƒ¼ãƒ«ã®ã¿è¨±å¯
        """
        # è¨€èªæŒ‡ç¤ºã‚’ system prompt æœ«å°¾ã«è¿½åŠ ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰ã€‚
        # drawio ç”Ÿæˆã®ã‚ˆã†ã« Markdown æŒ‡ç¤ºãŒè‡´å‘½çš„ã«ãªã‚‹ç”¨é€”ã§ã¯ OFF ã«ã™ã‚‹ã€‚
        if append_language_instruction:
            lang_instruction = _t("ai.output_language")
            system_prompt = system_prompt.rstrip() + "\n\n" + lang_instruction + "\n"

        run_debug: dict[str, Any] = {
            "started_at": time.strftime("%Y-%m-%dT%H:%M:%S"),
            "model": model_id or self._model_id or MODEL,
            "mcp_servers": {"microsoftdocs": {"url": MCP_MICROSOFT_DOCS.get("url"), "type": MCP_MICROSOFT_DOCS.get("type")}},
            "tool_total": 0,
            "tool_counts": {},
            "errors": [],
        }
        started = time.monotonic()

        try:
            # 1. SDK æ¥ç¶šï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¸ˆã¿ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’å†åˆ©ç”¨ï¼‰
            client = await _get_or_create_client(on_status=self._on_status)

            # 2. ã‚»ãƒƒã‚·ãƒ§ãƒ³ä½œæˆï¼ˆhooks ãƒ‘ã‚¿ãƒ¼ãƒ³ + MCP ã‚µãƒ¼ãƒãƒ¼ï¼‰
            session_cfg: dict[str, Any] = {
                "model": model_id or self._model_id or MODEL,
                "streaming": True,
                "on_permission_request": _approve_all,
                "system_message": system_prompt,
                # Tool visibility hint: some environments require explicit allow-listing.
                # Keep this minimal and still enforce decisions via on_pre_tool_use.
                "available_tools": [
                    "microsoft_docs_search",
                    "microsoft_docs_fetch",
                    "microsoft_code_sample_search",
                ],
                "hooks": {
                    "on_pre_tool_use": _make_on_pre_tool_use(on_status=self._on_status, run_debug=run_debug),
                    "on_error_occurred": _make_error_handler(self._on_status, run_debug=run_debug),
                },
            }

            # Microsoft Docs MCP ã‚’ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«æ¥ç¶š
            # learn.microsoft.com/api/mcp â†’ AI ãŒè‡ªå¾‹çš„ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢å¯èƒ½
            session_cfg["mcp_servers"] = {
                "microsoftdocs": MCP_MICROSOFT_DOCS,
            }
            self._on_status("Connecting Microsoft Docs MCP... (https://learn.microsoft.com/api/mcp)" if get_language() == "en" else "Microsoft Docs MCP ã‚’æ¥ç¶šä¸­... (https://learn.microsoft.com/api/mcp)")

            session = await client.create_session(session_cfg)

            # 3. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã‚¤ãƒ™ãƒ³ãƒˆåé›†ï¼ˆsession.idle ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
            collected: list[str] = []
            done = asyncio.Event()
            reasoning_notified = False

            def _handler(event: Any) -> None:
                # doneå¾Œã«é…å»¶ã‚¤ãƒ™ãƒ³ãƒˆãŒåˆ°ç€ã—ã¦ã‚‚å®‰å…¨ã«ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹ (review #7)
                if done.is_set():
                    return
                etype = event.type.value if hasattr(event.type, "value") else str(event.type)

                # Capture session info about tool availability (best-effort)
                try:
                    allowed = getattr(event.data, "allowed_tools", None)
                    if allowed is not None and "allowed_tools" not in run_debug:
                        run_debug["allowed_tools"] = list(allowed) if isinstance(allowed, list) else allowed
                        if isinstance(allowed, list):
                            self._on_status(f"Allowed tools: {len(allowed)}")

                    telemetry = getattr(event.data, "tool_telemetry", None)
                    if telemetry is not None and "tool_telemetry" not in run_debug:
                        run_debug["tool_telemetry"] = telemetry
                except Exception:
                    pass

                if etype == "assistant.message_delta":
                    delta = getattr(event.data, "delta_content", "")
                    if delta:
                        collected.append(delta)
                        self._on_delta(delta)

                elif etype == "tool.execution_start":
                    # Tool execution started (includes MCP tool name if applicable)
                    try:
                        tool_name = getattr(event.data, "tool_name", None)
                        mcp_server = getattr(event.data, "mcp_server_name", None)
                        mcp_tool = getattr(event.data, "mcp_tool_name", None)
                        run_debug.setdefault("tool_exec", []).append({
                            "tool_name": tool_name,
                            "mcp_server": mcp_server,
                            "mcp_tool": mcp_tool,
                        })
                        if mcp_tool:
                            self._on_status(f"Tool exec start: {mcp_server}:{mcp_tool}")
                        elif tool_name:
                            self._on_status(f"Tool exec start: {tool_name}")
                    except Exception:
                        pass

                elif etype == "assistant.reasoning_delta":
                    # æ¨è«–éç¨‹ï¼ˆchain-of-thoughtï¼‰ã‚’ãã®ã¾ã¾è¡¨ç¤ºã—ãªã„
                    nonlocal reasoning_notified
                    if not reasoning_notified:
                        reasoning_notified = True
                        self._on_status("AI thinking..." if get_language() == "en" else "AI æ€è€ƒä¸­...")

                elif etype == "assistant.message":
                    # æœ€çµ‚ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆstreaming ã®æœ‰ç„¡ã«é–¢ã‚ã‚‰ãšé€ä¿¡ã•ã‚Œã‚‹ï¼‰
                    content = getattr(event.data, "content", "")
                    if content and not collected:
                        collected.append(content)

                elif etype == "session.idle":
                    # ã‚»ãƒƒã‚·ãƒ§ãƒ³å®Œäº†ã‚·ã‚°ãƒŠãƒ«
                    done.set()

            session.on(_handler)

            # 4. é€ä¿¡ï¼ˆsend + idle å¾…ã¡ â€” SDK æ¨å¥¨ãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
            self._on_status("AI processing..." if get_language() == "en" else "AI å‡¦ç†å®Ÿè¡Œä¸­...")
            await session.send({"prompt": prompt})

            # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆä»˜ãã§ idle å¾…ã¡ï¼ˆé•·æ™‚é–“ã‚¿ã‚¹ã‚¯ã¯ heartbeat ã§é€²æ—è¡¨ç¤ºï¼‰
            effective_timeout = float(timeout_s) if timeout_s is not None else float(SEND_TIMEOUT)
            hb = float(heartbeat_s) if heartbeat_s is not None else 0.0
            try:
                if hb > 0:
                    while True:
                        elapsed = time.monotonic() - started
                        remaining = effective_timeout - elapsed
                        if remaining <= 0:
                            raise asyncio.TimeoutError
                        chunk = hb if remaining > hb else remaining
                        try:
                            await asyncio.wait_for(done.wait(), timeout=chunk)
                            break
                        except asyncio.TimeoutError:
                            elapsed2 = time.monotonic() - started
                            mins = int(elapsed2 // 60)
                            if get_language() == "en":
                                self._on_status(f"AI still running... (elapsed {mins} min)")
                            else:
                                self._on_status(f"AI å‡¦ç†å®Ÿè¡Œä¸­...ï¼ˆçµŒé {mins}åˆ†ï¼‰")
                else:
                    await asyncio.wait_for(done.wait(), timeout=effective_timeout)
            except asyncio.TimeoutError:
                if get_language() == "en":
                    self._on_status(f"AI timed out ({effective_timeout:g}s)")
                else:
                    self._on_status(f"AI å‡¦ç†ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ{effective_timeout:g}ç§’ï¼‰")

            result = "".join(collected) if collected else None

            # 5. ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ã¿ç ´æ£„ï¼ˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç¶­æŒï¼‰
            await session.destroy()

            # ãƒ„ãƒ¼ãƒ«åˆ©ç”¨ã‚µãƒãƒªï¼ˆGUIãƒ­ã‚°å‘ã‘ï¼‰
            try:
                tc: dict[str, dict[str, int]] = run_debug.get("tool_counts", {})  # type: ignore[assignment]
                docs_allow = 0
                docs_deny = 0
                for name, cnt in tc.items():
                    if str(name).startswith("microsoft_"):
                        docs_allow += int(cnt.get("allow", 0))
                        docs_deny += int(cnt.get("deny", 0))
                self._on_status(
                    f"Tool summary: total={run_debug.get('tool_total', 0)}, microsoft_docs_allow={docs_allow}, microsoft_docs_deny={docs_deny}"
                )
            except Exception:
                pass

            run_debug["duration_s"] = round(time.monotonic() - started, 3)
            run_debug["result_chars"] = len(result or "")
            _set_last_run_debug(run_debug)

            return result

        except Exception as e:
            self._on_status(f"AI review error: {e}" if get_language() == "en" else f"AI ãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
            run_debug["duration_s"] = round(time.monotonic() - started, 3)
            run_debug["exception"] = str(e)[:500]
            _set_last_run_debug(run_debug)
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡åŠ¹åŒ–ï¼ˆæ¬¡å›å†ä½œæˆï¼‰
            _invalidate_cached_client()
            return None


def _invalidate_cached_client() -> None:
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¸ˆã¿ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã‚’ã‚¹ãƒ¬ãƒƒãƒ‰ã‚»ãƒ¼ãƒ•ã«ç„¡åŠ¹åŒ–ã™ã‚‹ã€‚"""
    global _cached_client, _cached_client_started
    with _bg_lock:
        _cached_client = None
        _cached_client_started = False


# ============================================================
# åŒæœŸãƒ©ãƒƒãƒ‘ãƒ¼ï¼ˆtkinter ã‚¹ãƒ¬ãƒƒãƒ‰ã‹ã‚‰å‘¼ã¶ç”¨ï¼‰
# ============================================================


def _extract_resource_types(resource_text: str) -> list[str]:
    """ãƒªã‚½ãƒ¼ã‚¹ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ type åˆ—ã‚’æŠ½å‡ºã™ã‚‹ï¼ˆãƒ™ã‚¹ãƒˆã‚¨ãƒ•ã‚©ãƒ¼ãƒˆï¼‰ã€‚"""
    types: set[str] = set()
    for line in resource_text.splitlines():
        parts = line.split()
        for p in parts:
            if "/" in p and p.lower().startswith("microsoft."):
                types.add(p.strip().lower())
    return list(types)


def run_ai_review(
    resource_text: str,
    on_delta: Optional[Callable[[str], None]] = None,
    on_status: Optional[Callable[[str], None]] = None,
    model_id: str | None = None,
) -> str | None:
    """åŒæœŸçš„ã«AIãƒ¬ãƒ“ãƒ¥ãƒ¼ã‚’å®Ÿè¡Œã—ã¦çµæœã‚’è¿”ã™ã€‚"""
    reviewer = AIReviewer(on_delta=on_delta, on_status=on_status, model_id=model_id)
    return _run_async(reviewer.review(resource_text))


def run_security_report(
    security_data: dict,
    resource_text: str,
    template: dict | None = None,
    custom_instruction: str = "",
    on_delta: Optional[Callable[[str], None]] = None,
    on_status: Optional[Callable[[str], None]] = None,
    model_id: str | None = None,
    subscription_info: str = "",
) -> str | None:
    """ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã€‚"""
    resource_types = _extract_resource_types(resource_text)
    data_sections: list[tuple[str, str, dict]] = [
        ("Security Data", "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ‡ãƒ¼ã‚¿", security_data),
    ]
    return _run_report(
        base_system_prompt=_system_prompt_security_base(),
        report_type="security",
        data_sections=data_sections,
        resource_text=resource_text,
        resource_types=resource_types,
        search_queries_fn=security_search_queries,
        template=template,
        custom_instruction=custom_instruction,
        on_delta=on_delta,
        on_status=on_status,
        model_id=model_id,
        subscription_info=subscription_info,
    )


def run_cost_report(
    cost_data: dict,
    advisor_data: dict,
    template: dict | None = None,
    custom_instruction: str = "",
    on_delta: Optional[Callable[[str], None]] = None,
    on_status: Optional[Callable[[str], None]] = None,
    resource_types: list[str] | None = None,
    model_id: str | None = None,
    subscription_info: str = "",
) -> str | None:
    """ã‚³ã‚¹ãƒˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã€‚"""
    data_sections: list[tuple[str, str, dict]] = [
        ("Cost Data", "ã‚³ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿", cost_data),
        ("Advisor Recommendations", "Advisor æ¨å¥¨äº‹é …", advisor_data),
    ]
    return _run_report(
        base_system_prompt=_system_prompt_cost_base(),
        report_type="cost",
        data_sections=data_sections,
        resource_text=None,
        resource_types=resource_types or [],
        search_queries_fn=cost_search_queries,
        template=template,
        custom_instruction=custom_instruction,
        on_delta=on_delta,
        on_status=on_status,
        model_id=model_id,
        subscription_info=subscription_info,
    )


def run_summary_report(
    report_contents: list[tuple[str, str]],
    on_delta: Optional[Callable[[str], None]] = None,
    on_status: Optional[Callable[[str], None]] = None,
    model_id: str | None = None,
    subscription_info: str = "",
) -> str | None:
    """è¤‡æ•°ãƒ¬ãƒãƒ¼ãƒˆã®ã‚µãƒãƒªï¼ˆã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªï¼‰ã‚’ç”Ÿæˆã€‚

    Args:
        report_contents: [(report_type, markdown_text), ...] ä¾‹: [("security", "..."), ("cost", "...")]
    """
    reviewer = AIReviewer(on_delta=on_delta, on_status=on_status, model_id=model_id)

    en = get_language() == "en"
    if en:
        system_prompt = (
            "You are an Azure operations expert.\n"
            "The user has generated multiple Azure reports (security, cost, etc.).\n"
            "Your task is to produce a concise Executive Summary that:\n"
            "1. Highlights the most critical findings across ALL reports\n"
            "2. Provides a unified risk/opportunity matrix (Critical / High / Medium / Low)\n"
            "3. Recommends top 5 priority actions with estimated effort (Quick win / Strategic)\n"
            "4. Keeps total length under 800 words\n\n"
            "Output in Markdown. Do not repeat the full reports â€” summarize and cross-reference.\n"
            "Do not mention internal tools or tool errors.\n"
        )
    else:
        system_prompt = (
            "ã‚ãªãŸã¯ Azure é‹ç”¨ã®å°‚é–€å®¶ã§ã™ã€‚\n"
            "ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒè¤‡æ•°ã® Azure ãƒ¬ãƒãƒ¼ãƒˆï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ã€ã‚³ã‚¹ãƒˆç­‰ï¼‰ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚\n"
            "ä»¥ä¸‹ã®ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:\n"
            "1. å…¨ãƒ¬ãƒãƒ¼ãƒˆã‚’æ¨ªæ–­ã—ãŸæœ€é‡è¦æ‰€è¦‹ã‚’ãƒã‚¤ãƒ©ã‚¤ãƒˆ\n"
            "2. çµ±åˆãƒªã‚¹ã‚¯/æ©Ÿä¼šãƒãƒˆãƒªã‚¯ã‚¹ï¼ˆCritical / High / Medium / Lowï¼‰\n"
            "3. å„ªå…ˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³ Top 5ï¼ˆå·¥æ•°ç›®å®‰: Quick win / Strategic ã‚’ä»˜è¨˜ï¼‰\n"
            "4. å…¨ä½“ 800 æ–‡å­—ä»¥å†…ã«åã‚ã‚‹\n\n"
            "Markdown ã§å‡ºåŠ›ã€‚å„ãƒ¬ãƒãƒ¼ãƒˆã®å…¨æ–‡ã¯ç¹°ã‚Šè¿”ã•ãšã€è¦ç´„ãƒ»ç›¸äº’å‚ç…§ã™ã‚‹ã“ã¨ã€‚\n"
            "å†…éƒ¨ãƒ„ãƒ¼ãƒ«ã®æœ‰ç„¡ãƒ»ã‚¢ã‚¯ã‚»ã‚¹å¯å¦ã«ã¯ä¸€åˆ‡è§¦ã‚Œãªã„ã“ã¨ã€‚\n"
        )

    parts: list[str] = []
    if subscription_info:
        label = "Target Subscription" if en else "å¯¾è±¡ã‚µãƒ–ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³"
        parts.append(f"**{label}**: {subscription_info}\n\n")

    header = "Generate an Executive Summary from the following reports." if en else "ä»¥ä¸‹ã®ãƒ¬ãƒãƒ¼ãƒˆç¾¤ã‹ã‚‰ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"
    parts.append(header + "\n\n")

    for rtype, content in report_contents:
        parts.append(f"## {rtype.upper()} Report\n\n{content}\n\n---\n\n")

    prompt = "".join(parts)
    return _run_async(
        reviewer.generate(prompt, system_prompt, model_id=model_id),
        timeout_s=REPORT_SEND_TIMEOUT + 30,
    )


def run_integrated_report(
    *,
    diagram_summaries: list[dict[str, Any]],
    report_contents: list[tuple[str, str]],
    diff_contents: list[tuple[str, str]] | None = None,
    on_delta: Optional[Callable[[str], None]] = None,
    on_status: Optional[Callable[[str], None]] = None,
    model_id: str | None = None,
    subscription_info: str = "",
    resource_group: str = "",
) -> str | None:
    """è¤‡æ•°ãƒ“ãƒ¥ãƒ¼ï¼ˆå›³/ãƒ¬ãƒãƒ¼ãƒˆï¼‰ã‚’æ¨ªæ–­ã—ãŸçµ±åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã™ã‚‹ã€‚"""
    reviewer = AIReviewer(on_delta=on_delta, on_status=on_status, model_id=model_id)

    en = get_language() == "en"
    if en:
        system_prompt = (
            "You are an Azure operations expert.\n"
            "The user generated multiple outputs from an Azure environment: diagrams and/or reports.\n"
            "Your task is to produce ONE integrated report in Markdown that:\n"
            "1. Summarizes environment overview from diagram summaries\n"
            "2. Summarizes key findings from each report (security/cost)\n"
            "3. Adds cross-domain insights (security vs cost vs architecture)\n"
            "4. Provides a unified priority action list (Top 5) with effort labels\n"
            "If diff data is provided, summarize what changed (3-5 bullets).\n"
            "Do not repeat the full reports. Be specific to the provided data.\n"
            "Do not mention internal tools, tool availability, or any tool errors.\n"
            "\n"
            "CRITICAL: Output the Markdown report DIRECTLY as plain text.\n"
            "Do NOT wrap the output in <tool_calls>, <tool_call>, <tool_input>, or any XML tags.\n"
            "Do NOT simulate file-creation tool calls. Just output the Markdown text directly.\n"
        )
    else:
        system_prompt = (
            "ã‚ãªãŸã¯ Azure é‹ç”¨ã®å°‚é–€å®¶ã§ã™ã€‚\n"
            "ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒ Azure ç’°å¢ƒã‹ã‚‰è¤‡æ•°ã®å‡ºåŠ›ï¼ˆå›³ã‚µãƒãƒª / ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ / ã‚³ã‚¹ãƒˆç­‰ï¼‰ã‚’ç”Ÿæˆã—ã¾ã—ãŸã€‚\n"
            "Markdown ã§ã€çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã€ã‚’1æœ¬ä½œã£ã¦ãã ã•ã„ã€‚è¦ä»¶:\n"
            "1. å›³ã‚µãƒãƒªã‹ã‚‰ç’°å¢ƒæ¦‚è¦ï¼ˆæ§‹æˆ/è¦æ¨¡ï¼‰ã‚’è¦ç´„\n"
            "2. å„ãƒ¬ãƒãƒ¼ãƒˆï¼ˆsecurity/costï¼‰ã®é‡è¦æ‰€è¦‹ã‚’è¦ç´„\n"
            "3. æ¨ªæ–­çš„ãªæ´å¯Ÿï¼ˆã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£Ã—ã‚³ã‚¹ãƒˆÃ—ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ï¼‰ã‚’è¿½åŠ \n"
            "4. å„ªå…ˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³ Top 5ï¼ˆå·¥æ•°ç›®å®‰: Quick win / Strategicï¼‰\n"
            "å·®åˆ†ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã¯ã€å‰å›ã‹ã‚‰ä½•ãŒå¤‰ã‚ã£ãŸã‹ã‚’ 3ã€œ5 ç‚¹ã§è¦ç´„\n"
            "å…¨æ–‡ã®è²¼ã‚Šç›´ã—ã¯é¿ã‘ã€è¦ç´„ä¸­å¿ƒã§å…·ä½“çš„ã«ã€‚\n"
            "å†…éƒ¨ãƒ„ãƒ¼ãƒ«ã®æœ‰ç„¡ãƒ»ã‚¢ã‚¯ã‚»ã‚¹å¯å¦ãƒ»ãƒ„ãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼ç­‰ã«ã¯ä¸€åˆ‡è§¦ã‚Œãªã„ã§ãã ã•ã„ã€‚\n"
            "\n"
            "é‡è¦: ãƒ¬ãƒãƒ¼ãƒˆã¯ Markdown ãƒ†ã‚­ã‚¹ãƒˆã¨ã—ã¦ç›´æ¥å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n"
            "<tool_calls>/<tool_call>/<tool_input> ç­‰ã® XML ã‚¿ã‚°ã§å›²ã¾ãªã„ã§ãã ã•ã„ã€‚\n"
            "ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆã®ãƒ„ãƒ¼ãƒ«å‘¼ã³å‡ºã—ã‚’æ¨¡å€£ã—ãªã„ã§ãã ã•ã„ã€‚Markdown ã‚’ãã®ã¾ã¾å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚\n"
        )

    parts: list[str] = []
    if subscription_info:
        label = "Target Subscription" if en else "å¯¾è±¡ã‚µãƒ–ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³"
        parts.append(f"**{label}**: {subscription_info}\n")
    if resource_group:
        label = "Target Resource Group" if en else "å¯¾è±¡ãƒªã‚½ãƒ¼ã‚¹ã‚°ãƒ«ãƒ¼ãƒ—"
        parts.append(f"**{label}**: {resource_group}\n")
    if parts:
        parts.append("\n")

    header = "Generate an integrated Azure operations report from the following inputs." if en else "ä»¥ä¸‹ã®å…¥åŠ›ã‹ã‚‰ Azure é‹ç”¨ã®çµ±åˆãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"
    parts.append(header + "\n\n")

    if diagram_summaries:
        title = "Diagram Summaries" if en else "å›³ã‚µãƒãƒª"
        parts.append(f"## {title}\n")
        parts.append("```json\n" + json.dumps(diagram_summaries, ensure_ascii=False) + "\n```\n\n")

    for rtype, content in report_contents:
        parts.append(f"## {rtype.upper()} Report\n\n{content}\n\n---\n\n")

    if diff_contents:
        diff_title = "Changes from Previous Reports" if en else "å‰å›ãƒ¬ãƒãƒ¼ãƒˆã‹ã‚‰ã®å¤‰æ›´ç‚¹"
        parts.append(f"## {diff_title}\n\n")
        for rtype, diff_md in diff_contents:
            parts.append(f"### {rtype.upper()} Diff\n\n{diff_md}\n\n---\n\n")

    prompt = "".join(parts)
    raw = _run_async(
        reviewer.generate(prompt, system_prompt, model_id=model_id,
                         timeout_s=REPORT_SEND_TIMEOUT),
        timeout_s=REPORT_SEND_TIMEOUT + 30,
    )

    if not isinstance(raw, str):
        return raw

    sanitized = _sanitize_ai_markdown(raw)

    # If sanitization removed headings but the content is otherwise present, salvage by
    # prepending a top-level heading to avoid unnecessary fallback.
    if sanitized and not any(l.lstrip().startswith("#") for l in sanitized.splitlines() if l.strip()):
        title = "# Integrated Report" if get_language() == "en" else "# çµ±åˆãƒ¬ãƒãƒ¼ãƒˆ"
        sanitized = title + "\n\n" + sanitized.lstrip()

    # ã¾ã ãƒ„ãƒ¼ãƒ«ç—•è·¡ãŒæ®‹ã‚‹/è¦‹å‡ºã—ãŒç„¡ã„å ´åˆã¯ã€Œçµ±åˆã¨ã—ã¦ä¸æ­£ã€ã¨ã—ã¦æ‰±ã†
    lowered = sanitized.lower()
    reasons: list[str] = []
    if (
        "<tool_" in lowered
        or "<parameters" in lowered
        or "<parameter" in lowered
        or "<result" in lowered
        or "<filereadresult" in lowered
        or "<filewriteresult" in lowered
    ):
        reasons.append("tool_trace")
    if not any(l.lstrip().startswith("#") for l in sanitized.splitlines() if l.strip()):
        reasons.append("no_heading")

    if reasons:
        log = on_status or (lambda _s: None)
        msg = (
            "AI output invalid for integrated report" if en else "AI çµ±åˆå‡ºåŠ›ãŒä¸æ­£ã§ã™"
        ) + f" ({', '.join(reasons)})"
        log(msg)
        # Best-effort debug hint (keep short to avoid log bloat)
        try:
            raw_preview = raw.replace("\r", "").replace("\n", "\\n")[:240]
            san_preview = sanitized.replace("\r", "").replace("\n", "\\n")[:240]
            if en:
                log(f"  raw(head): {raw_preview}")
                log(f"  sanitized(head): {san_preview}")
            else:
                log(f"  raw(å…ˆé ­): {raw_preview}")
                log(f"  sanitized(å…ˆé ­): {san_preview}")
        except Exception:
            pass
        return None

    return sanitized


def run_drawio_generation(
    diagram_request: dict[str, Any],
    *,
    on_delta: Optional[Callable[[str], None]] = None,
    on_status: Optional[Callable[[str], None]] = None,
    model_id: str | None = None,
    require_azure2_icons: bool = True,
    max_attempts: int = 3,
) -> str | None:
    """AI ã§ draw.io (mxfile) XML ã‚’ç”Ÿæˆã—ã€ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã—ã¦è¿”ã™ã€‚

    - å‡ºåŠ›ã¯ XML ã®ã¿ã‚’æœŸå¾…ã™ã‚‹ãŒã€é€¸è„±ã—ãŸå ´åˆã¯æŠ½å‡ºã§æ•‘æ¸ˆã™ã‚‹ã€‚
    - ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ERRORãŒå‡ºãŸå ´åˆã¯æœ€å¤§ *max_attempts* å›ãƒªãƒˆãƒ©ã‚¤ã€‚
    - ã™ã¹ã¦å¤±æ•—ã—ãŸå ´åˆã¯ Noneã€‚
    """
    from .drawio_validate import Issue, validate_drawio_xml

    import xml.etree.ElementTree as ET

    log = on_status or (lambda _s: None)

    base_prompt = (
        "Generate a draw.io diagram from the following JSON." if get_language() == "en" else "ä»¥ä¸‹ã®JSONã‹ã‚‰ draw.io å›³ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"
    )
    # NOTE: keep this compact to reduce token usage when nodes are many.
    prompt = base_prompt + "\n\n```json\n" + json.dumps(diagram_request, ensure_ascii=False) + "\n```\n"

    reviewer = AIReviewer(
        on_delta=on_delta or (lambda _d: None),
        on_status=on_status,
        model_id=model_id,
    )

    system_prompt = _system_prompt_drawio()

    # Input-derived expectations (to prevent "blank" or container-only diagrams).
    node_cell_ids: list[str] = []
    try:
        for n in (diagram_request.get("nodes") or []):
            cid = n.get("cellId") if isinstance(n, dict) else None
            if isinstance(cid, str) and cid.strip():
                node_cell_ids.append(cid.strip())
    except Exception:
        node_cell_ids = []

    # Require at least some of the provided nodes to be present as mxCell ids.
    # Keep this lenient to avoid rejecting large diagrams due to token limits.
    min_present = 1
    if node_cell_ids:
        min_present = max(1, min(10, len(node_cell_ids) // 10))  # 10% up to 10 nodes

    last_issues: list[str] = []
    for attempt in range(1, max(1, int(max_attempts)) + 1):
        if attempt > 1:
            log(_t("log.ai_drawio_retry", attempt=attempt, max=max_attempts))

        run_prompt = prompt
        if last_issues:
            # ã‚¨ãƒ©ãƒ¼ã‚’ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã—ã¦å†ç”Ÿæˆ
            issues_block = "\n".join(f"- {x}" for x in last_issues[:20])
            if get_language() == "en":
                run_prompt += (
                    "\n\nValidation errors from the previous attempt:\n" + issues_block +
                    "\n\nRegenerate the FULL corrected mxfile XML. Output XML only."
                )
            else:
                run_prompt += (
                    "\n\nå‰å›ã®å‡ºåŠ›ã®ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼:\n" + issues_block +
                    "\n\nã‚¨ãƒ©ãƒ¼ã‚’è§£æ¶ˆã—ãŸå®Œå…¨ãª mxfile XML ã‚’å†ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚XMLã®ã¿å‡ºåŠ›ã€‚"
                )

        result = _run_async(
            reviewer.generate(
                run_prompt,
                system_prompt,
                model_id=model_id,
                append_language_instruction=False,
                timeout_s=DRAWIO_SEND_TIMEOUT,
                heartbeat_s=HEARTBEAT_INTERVAL,
            ),
            timeout_s=DRAWIO_SEND_TIMEOUT + 30,
        )
        if not result:
            last_issues = ["Empty model output"]
            continue

        xml = _extract_mxfile_xml(result)
        if not xml:
            last_issues = ["Could not find <mxfile>...</mxfile> in the output"]
            continue

        issues = validate_drawio_xml(xml, require_azure2_icons=require_azure2_icons)
        errors = [i for i in issues if i.level == "ERROR"]

        # Extra gate: ensure generated XML contains enough of the requested node ids.
        if not errors and node_cell_ids:
            try:
                root = ET.fromstring(xml)
                found_ids = {c.get("id") for c in root.findall(".//mxCell")}
                present = sum(1 for cid in node_cell_ids if cid in found_ids)
                total = len(node_cell_ids)
                log(_t("log.ai_drawio_stats", present=present, total=total))
                if present < min_present:
                    errors.append(Issue("ERROR", f"Too few input nodes present in XML: {present}/{total} (min {min_present})"))
            except Exception:
                errors.append(Issue("ERROR", "Failed to parse generated XML for node-coverage check"))

        if not errors:
            return xml

        log(_t("log.ai_drawio_validation_failed", count=len(errors)))
        last_issues = [e.message for e in errors]

    return None


# ============================================================
# å…±é€šãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ˜ãƒ«ãƒ‘
# ============================================================

def _run_report(
    *,
    base_system_prompt: str,
    report_type: str,
    data_sections: list[tuple[str, str, dict]],
    resource_text: str | None,
    resource_types: list[str],
    search_queries_fn: Callable,
    template: dict | None,
    custom_instruction: str,
    on_delta: Optional[Callable[[str], None]],
    on_status: Optional[Callable[[str], None]],
    model_id: str | None,
    subscription_info: str = "",
) -> str | None:
    """security / cost ãƒ¬ãƒãƒ¼ãƒˆ ã®å…±é€šãƒ­ã‚¸ãƒƒã‚¯ã€‚"""
    reviewer = AIReviewer(on_delta=on_delta, on_status=on_status, model_id=model_id)
    log = on_status or (lambda s: None)

    # ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ â†’ ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
    if template:
        tmpl_instruction = build_template_instruction(template, custom_instruction)
        system_prompt = base_system_prompt + "\n\n" + tmpl_instruction
    else:
        system_prompt = base_system_prompt
        if custom_instruction.strip():
            system_prompt += f"\n\n### ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‹ã‚‰ã®è¿½åŠ æŒ‡ç¤º:\n{custom_instruction.strip()}"

    # Microsoft Docs å‚ç…§
    queries = search_queries_fn(resource_types)
    docs_block = enrich_with_docs(
        queries, report_type=report_type,
        resource_types=resource_types, on_status=log,
    )
    if not docs_block:
        log("Microsoft Docs: generating report without references"
            if get_language() == "en"
            else "Microsoft Docs å‚ç…§ãªã—ã§ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¾ã™")

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆçµ„ã¿ç«‹ã¦
    en = get_language() == "en"
    parts: list[str] = []

    # ã‚µãƒ–ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³æƒ…å ±ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ã«ä½¿ãˆã‚‹ã‚ˆã†ã«ï¼‰
    if subscription_info:
        if en:
            parts.append(f"**Target Subscription**: {subscription_info}\n\n")
        else:
            parts.append(f"**å¯¾è±¡ã‚µãƒ–ã‚¹ã‚¯ãƒªãƒ—ã‚·ãƒ§ãƒ³**: {subscription_info}\n\n")

    if en:
        parts.append(
            f"Generate a {report_type} report for the following Azure environment.\n\n"
            "**Important**: Read the data below carefully and provide environment-specific findings.\n"
            "Reference specific resource names and types; avoid generic advice.\n"
            "If reference URLs are provided below, cite them where relevant.\n"
            "Do not mention internal tools, tool access, or any tool errors.\n"
        )
    else:
        parts.append(
            f"ä»¥ä¸‹ã® Azure ç’°å¢ƒã®{report_type}ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n\n"
            "**é‡è¦**: ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‚’ã‚ˆãèª­ã¿ã€ã“ã®ç’°å¢ƒå›ºæœ‰ã®å…·ä½“çš„ãªæŒ‡æ‘˜ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚\n"
            "ãƒªã‚½ãƒ¼ã‚¹åã‚„ã‚¿ã‚¤ãƒ—ã‚’å…·ä½“çš„ã«æŒ™ã’ã¦ã‚³ãƒ¡ãƒ³ãƒˆã—ã€ã€Œä¸€èˆ¬è«–ã€ã¯é¿ã‘ã¦ãã ã•ã„ã€‚\n"
            "ä»¥ä¸‹ã«å‚ç…§URLãŒæç¤ºã•ã‚Œã¦ã„ã‚Œã°ã€é©å®œå¼•ç”¨ã—ã¦ãã ã•ã„ã€‚\n"
            "å†…éƒ¨ãƒ„ãƒ¼ãƒ«ã®æœ‰ç„¡ãƒ»ã‚¢ã‚¯ã‚»ã‚¹å¯å¦ãƒ»ãƒ„ãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼ç­‰ã«ã¯ä¸€åˆ‡è§¦ã‚Œãªã„ã§ãã ã•ã„ã€‚\n"
        )

    for en_title, ja_title, data in data_sections:
        title = en_title if en else ja_title
        parts.append(f"\n## {title}\n```json\n{json.dumps(data, indent=2, ensure_ascii=False)}\n```\n")

    if resource_text:
        rt_title = "Resource List" if en else "ãƒªã‚½ãƒ¼ã‚¹ä¸€è¦§"
        parts.append(f"\n## {rt_title}\n```\n{resource_text}\n```")

    if docs_block:
        parts.append(docs_block)

    prompt = "".join(parts)
    return _run_async(
        reviewer.generate(prompt, system_prompt, model_id=model_id,
                         timeout_s=REPORT_SEND_TIMEOUT),
        timeout_s=REPORT_SEND_TIMEOUT + 30,
    )


def list_available_model_ids_sync(
    on_status: Optional[Callable[[str], None]] = None,
    timeout: float = 15,
) -> list[str]:
    """åˆ©ç”¨å¯èƒ½ãƒ¢ãƒ‡ãƒ«IDä¸€è¦§ã‚’åŒæœŸçš„ã«å–å¾—ã™ã‚‹ã€‚

    GUI ã®ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¹ãƒ¬ãƒƒãƒ‰ã‹ã‚‰å‘¼ã¹ã‚‹ã‚ˆã†ã«åŒæœŸåŒ–ã€‚
    *timeout* ç§’ã§æ¥ç¶š/å–å¾—ã§ããªã‘ã‚Œã°ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚
    """

    async def _inner() -> list[str]:
        client = await _get_or_create_client(on_status=on_status)
        return await _list_model_ids_async(client)

    future: concurrent.futures.Future[list[str]] | None = None
    try:
        loop = _get_bg_loop()
        future = asyncio.run_coroutine_threadsafe(_inner(), loop)
        return list(future.result(timeout=timeout))
    except concurrent.futures.TimeoutError:
        if on_status:
            on_status(f"Copilot SDK: Model listing timed out ({timeout:g}s)" if get_language() == "en" else f"Copilot SDK: ãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—ãŒ {timeout:g}s ã‚’è¶…éã—ã¾ã—ãŸ")
        if future is not None:
            try:
                future.cancel()
            except Exception:
                pass
        return []
    except Exception as exc:
        if on_status:
            on_status(f"Copilot SDK: Model listing error: {type(exc).__name__}: {exc}" if get_language() == "en" else f"Copilot SDK: ãƒ¢ãƒ‡ãƒ«ä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {type(exc).__name__}: {exc}")
        if future is not None:
            try:
                future.cancel()
            except Exception:
                pass
        return []


# ============================================================
# æ°¸ç¶šã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ï¼ˆCopilotClient ã®ãƒ©ã‚¤ãƒ•ã‚µã‚¤ã‚¯ãƒ«ã«åˆã‚ã›ã‚‹ï¼‰
# ============================================================

_bg_loop: asyncio.AbstractEventLoop | None = None
_bg_thread: threading.Thread | None = None
_bg_lock = threading.Lock()


def _get_bg_loop() -> asyncio.AbstractEventLoop:
    """CopilotClient å°‚ç”¨ã®æ°¸ç¶šã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ã‚’è¿”ã™ã€‚

    asyncio.run() ã¯æ¯å›ãƒ«ãƒ¼ãƒ—ã‚’é–‰ã˜ã¦ã—ã¾ã„ã€ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¸ˆã¿
    CopilotClient ãŒ 'Event loop is closed' ã§ã‚¯ãƒ©ãƒƒã‚·ãƒ¥ã™ã‚‹ãŸã‚ã€
    å°‚ç”¨ã‚¹ãƒ¬ãƒƒãƒ‰ã§ run_forever ã™ã‚‹æ°¸ç¶šãƒ«ãƒ¼ãƒ—ã‚’åˆ©ç”¨ã™ã‚‹ã€‚
    """
    global _bg_loop, _bg_thread
    with _bg_lock:
        if _bg_loop is not None and not _bg_loop.is_closed():
            return _bg_loop
        loop = asyncio.new_event_loop()

        loop_ready = threading.Event()

        def _run() -> None:
            asyncio.set_event_loop(loop)
            # Signal readiness only after the loop has started and can process callbacks.
            loop.call_soon(loop_ready.set)
            loop.run_forever()

            try:
                loop.close()
            except Exception:
                pass

        thread = threading.Thread(target=_run, daemon=True, name="copilot-event-loop")
        thread.start()

        # ãƒ«ãƒ¼ãƒ—ãŒèµ·å‹•ã—ãªã„å ´åˆã€ã“ã®å¾Œã® run_coroutine_threadsafe ãŒæ°¸ä¹…å¾…ã¡ã«ãªã‚Šå¾—ã‚‹ã€‚
        if not loop_ready.wait(timeout=5):
            try:
                loop.call_soon_threadsafe(loop.stop)
            except Exception:
                pass
            raise RuntimeError("Copilot background event loop failed to start within 5 seconds")

        _bg_loop = loop
        _bg_thread = thread
        return loop


def _run_async(coro: Any, timeout_s: float | None = None) -> Any:
    """ã‚³ãƒ«ãƒ¼ãƒãƒ³ã‚’æ°¸ç¶šã‚¤ãƒ™ãƒ³ãƒˆãƒ«ãƒ¼ãƒ—ä¸Šã§åŒæœŸçš„ã«å®Ÿè¡Œã™ã‚‹ã€‚

    ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ã‚¹ãƒ¬ãƒƒãƒ‰ï¼ˆtkinter ãƒ¯ãƒ¼ã‚«ãƒ¼ï¼‰ã‹ã‚‰å‘¼ã¶å‰æã€‚
    CopilotClient ã¯åŒä¸€ãƒ«ãƒ¼ãƒ—ä¸Šã«ç•™ã¾ã‚‹ãŸã‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå®‰å…¨ã«ä½¿ãˆã‚‹ã€‚
    """
    loop = _get_bg_loop()
    future = asyncio.run_coroutine_threadsafe(coro, loop)
    timeout = timeout_s if timeout_s is not None else (SEND_TIMEOUT + 30)
    try:
        return future.result(timeout=timeout)
    except (TimeoutError, concurrent.futures.TimeoutError):
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ã¯ã‚³ãƒ«ãƒ¼ãƒãƒ³ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¦ãƒªã‚½ãƒ¼ã‚¹ãƒªãƒ¼ã‚¯ã‚’é˜²ã
        future.cancel()
        raise
